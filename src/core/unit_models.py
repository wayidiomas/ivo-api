# src/core/unit_models.py - ATUALIZADO PARA PYDANTIC V2 COMPLETO
"""Modelos espec√≠ficos para o sistema IVO V2 com hierarquia Course ‚Üí Book ‚Üí Unit."""
from typing import List, Optional, Dict, Any, Union
from pydantic import BaseModel, Field, HttpUrl, field_validator, ValidationInfo, ValidationError, ConfigDict
from datetime import datetime
from fastapi import UploadFile
import re
import time        # Para timestamps
import json        # Para parsing JSON  
import uuid        # Para UUIDs
import logging     # Para logs
from pathlib import Path

from .enums import (
    CEFRLevel, LanguageVariant, UnitType, AimType, 
    TipStrategy, GrammarStrategy, AssessmentType, 
    UnitStatus, ContentType
)


# =============================================================================
# INPUT MODELS (Form Data) - ATUALIZADOS COM HIERARQUIA E PYDANTIC V2
# =============================================================================

class UnitCreateRequest(BaseModel):
    """Request para cria√ß√£o de unidade via form data - REQUER HIERARQUIA."""
    # HIERARQUIA OBRIGAT√ìRIA (novos campos)
    course_id: str = Field(..., description="ID do curso (obrigat√≥rio)")
    book_id: str = Field(..., description="ID do book (obrigat√≥rio)")
    
    # Dados da unidade (existentes)
    context: Optional[str] = Field(None, description="Contexto opcional da unidade")
    cefr_level: CEFRLevel = Field(..., description="N√≠vel CEFR")
    language_variant: LanguageVariant = Field(..., description="Variante do idioma")
    unit_type: UnitType = Field(..., description="Tipo de unidade (lexical ou grammar)")
    
    @field_validator('book_id')
    @classmethod
    def validate_book_not_empty(cls, v: str) -> str:
        if not v or v.strip() == "":
            raise ValueError("book_id √© obrigat√≥rio")
        return v
    
    @field_validator('course_id')
    @classmethod
    def validate_course_not_empty(cls, v: str) -> str:
        if not v or v.strip() == "":
            raise ValueError("course_id √© obrigat√≥rio")
        return v
    
    model_config = {
        "json_schema_extra": {
            "example": {
                "course_id": "course_english_beginners",
                "book_id": "book_foundation_a1",
                "context": "Hotel reservation and check-in procedures",
                "cefr_level": "B1",
                "language_variant": "american_english",
                "unit_type": "lexical_unit"
            }
        }
    }


# =============================================================================
# VOCABULARY MODELS - ATUALIZADO COM VALIDA√á√ÉO IPA COMPLETA E PYDANTIC V2
# =============================================================================

class VocabularyItem(BaseModel):
    """Item de vocabul√°rio com fonema IPA validado - VERS√ÉO COMPLETA PYDANTIC V2."""
    word: str = Field(..., min_length=1, max_length=50, description="Palavra no idioma alvo")
    phoneme: str = Field(..., description="Transcri√ß√£o fon√©tica IPA v√°lida")
    definition: str = Field(..., min_length=1, max_length=200, description="Definition in English")
    example: str = Field(..., min_length=1, max_length=300, description="Example of usage in context")
    word_class: str = Field(..., description="Classe gramatical")
    frequency_level: str = Field("medium", description="N√≠vel de frequ√™ncia")
    
    # NOVOS CAMPOS PARA PROGRESS√ÉO
    context_relevance: Optional[float] = Field(None, ge=0.0, le=1.0, description="Relev√¢ncia contextual")
    is_reinforcement: Optional[bool] = Field(False, description="√â palavra de refor√ßo?")
    first_introduced_unit: Optional[str] = Field(None, description="Unidade onde foi introduzida")
    
    # NOVOS CAMPOS PARA IPA E FONEMAS
    ipa_variant: str = Field("general_american", description="Variante IPA")
    stress_pattern: Optional[str] = Field(None, description="Padr√£o de stress")
    syllable_count: Optional[int] = Field(None, ge=1, le=8, description="N√∫mero de s√≠labas")
    alternative_pronunciations: List[str] = Field(default=[], description="Pron√∫ncias alternativas")
    
    @field_validator('phoneme')
    @classmethod
    def validate_ipa_phoneme(cls, v: str) -> str:
        """Validar que o fonema usa s√≠mbolos IPA v√°lidos - FLEX√çVEL."""
        if not v:
            raise ValueError("Fonema √© obrigat√≥rio")
        
        # Verificar se est√° entre delimitadores IPA corretos
        if not ((v.startswith('/') and v.endswith('/')) or 
                (v.startswith('[') and v.endswith(']'))):
            raise ValueError("Fonema deve estar entre / / (fon√™mico) ou [ ] (fon√©tico)")
        
        # S√çMBOLOS IPA COMPLETOS + diacr√≠ticos + separadores para frases
        valid_ipa_chars = set(
            # Vogais b√°sicas e avan√ßadas (incluindo todas as vogais standard)
            'a√¶…ô…ë…í…î…™…õ…ú…ù…®…â ä å èyeioubcdf…°hijklmnpqrstu…•vwxyzAEIOU…™…õe…î…í…ë å ä…ô…ú…® è…â√¶'
            # R-colored vowels (importantes para ingl√™s americano)
            '…ö…ù'  # …ö = r-colored schwa, …ù = r-colored mid-central vowel
            # Consoantes especiais
            'Œ∏√∞ É í ß §≈ã…π…ª…æ…∏Œ≤√ß ù…† î…≤…üc…ï ë…ñ…≠…Ω à…≥ Ç ê…ª ã'
            # S√çMBOLOS FALTANTES IDENTIFICADOS (ingl√™s americano)
            '…ë'  # Vogal /…ë/ em "father" (j√° presente mas garantindo)
            # Consoantes latinas b√°sicas (fallback para casos de emergency)
            'gptk'  # Adicionado para fallback de emergency
            # Diacr√≠ticos e modificadores
            ' ∞ ∑ ≤À§Ã•Ã©ÃØÃ∞ÃπÃúÃüÃòÃôÃûÃ†ÃÉÃä'
            # Suprassegmentais
            'ÀàÀåÀêÀë'
            # Articula√ß√£o
            'Ã™Ã∫ÃªÃºÃùÃûÃòÃôÃóÃñÃØÃ∞Ã±ÃúÃüÃö'
            # Caracteres especiais permitidos (expandidos)
            ' .Àê-'  # Adicionado h√≠fen para palavras compostas
        )
        
        # Remover delimitadores para valida√ß√£o
        clean_phoneme = v.strip('/[]')
        
        # Verificar se cont√©m apenas s√≠mbolos IPA v√°lidos
        invalid_chars = set(clean_phoneme) - valid_ipa_chars
        if invalid_chars:
            raise ValueError(f"S√≠mbolos IPA inv√°lidos encontrados: {invalid_chars}")
        
        # Verificar padr√µes comuns de erro
        if '//' in clean_phoneme or '[[' in clean_phoneme:
            raise ValueError("Delimitadores duplicados n√£o s√£o permitidos")
        
        # Verificar se tem pelo menos um som v√°lido
        if len(clean_phoneme.strip()) == 0:
            raise ValueError("Fonema n√£o pode estar vazio")
        
        return v
    
    @field_validator('word')
    @classmethod
    def validate_word_format(cls, v: str) -> str:
        """Validar formato da palavra/express√£o - FLEX√çVEL para m√©todo direto."""
        if not v:
            raise ValueError("Palavra √© obrigat√≥ria")
        
        # M√âTODO DIRETO: Aceitar chunks, phrasal verbs, collocations
        # Permitir: letras, espa√ßos, h√≠fens, ap√≥strofes, pontos
        # Exemplos: "check in", "key card", "dining room", "mother-in-law", "don't"
        if not re.match(r"^[a-zA-Z\s\-'\.]+$", v.strip()):
            raise ValueError("Express√£o deve conter apenas letras, espa√ßos, h√≠fens, ap√≥strofes ou pontos")
        
        # Limpar espa√ßos extras mas manter estrutura
        cleaned = ' '.join(v.strip().split())
        return cleaned.lower()
    
    @field_validator('word_class')
    @classmethod
    def validate_word_class(cls, v: str) -> str:
        """Validar classe gramatical - EXPANDIDA para padr√µes nativos."""
        valid_classes = {
            # Classes b√°sicas
            "noun", "verb", "adjective", "adverb", "preposition", 
            "conjunction", "article", "pronoun", "interjection",
            "modal", "auxiliary", "determiner", "numeral",
            # M√âTODO DIRETO: Padr√µes naturais do ingl√™s
            "phrasal verb", "verb phrase", "noun phrase", "compound noun",
            "collocation", "idiom", "expression", "chunk", "phrase",
            "prepositional phrase", "adverbial phrase", "adjective phrase",
            "fixed expression", "compound adjective", "question_word"
        }
        
        if v.lower() not in valid_classes:
            raise ValueError(f"Classe gramatical deve ser uma de: {', '.join(sorted(valid_classes))}")
        
        return v.lower()
    
    @field_validator('frequency_level')
    @classmethod
    def validate_frequency_level(cls, v: str) -> str:
        """Validar n√≠vel de frequ√™ncia."""
        valid_levels = {"high", "medium", "low", "very_high", "very_low"}
        
        if v.lower() not in valid_levels:
            raise ValueError(f"N√≠vel de frequ√™ncia deve ser um de: {', '.join(valid_levels)}")
        
        return v.lower()
    
    @field_validator('ipa_variant')
    @classmethod
    def validate_ipa_variant(cls, v: str) -> str:
        """Validar variante IPA."""
        valid_variants = {
            "general_american", "received_pronunciation", "australian_english",
            "canadian_english", "irish_english", "scottish_english"
        }
        
        if v.lower() not in valid_variants:
            raise ValueError(f"Variante IPA deve ser uma de: {', '.join(valid_variants)}")
        
        return v.lower()
    
    @field_validator('alternative_pronunciations')
    @classmethod
    def validate_alternative_pronunciations(cls, v: List[str]) -> List[str]:
        """Validar pron√∫ncias alternativas."""
        # Aplicar a mesma valida√ß√£o IPA para cada item
        for pronunciation in v:
            if pronunciation and not ((pronunciation.startswith('/') and pronunciation.endswith('/')) or 
                                    (pronunciation.startswith('[') and pronunciation.endswith(']'))):
                raise ValueError("Pron√∫ncia alternativa deve seguir formato IPA")
        return v
    
    model_config = {
        "json_schema_extra": {
            "example": {
                "word": "restaurant",
                "phoneme": "/Ààr…õst…ôr…ënt/",
                "definition": "estabelecimento comercial onde se servem refei√ß√µes",
                "example": "We had dinner at a lovely Italian restaurant last night.",
                "word_class": "noun",
                "frequency_level": "high",
                "context_relevance": 0.95,
                "is_reinforcement": False,
                "ipa_variant": "general_american",
                "stress_pattern": "primary_first",
                "syllable_count": 3,
                "alternative_pronunciations": ["/Ààrest…ôr…ënt/"]
            }
        }
    }


class VocabularySection(BaseModel):
    """Se√ß√£o completa de vocabul√°rio - ATUALIZADA COM RAG E VALIDA√á√ÉO PYDANTIC V2."""
    model_config = ConfigDict(json_encoders={datetime: lambda v: v.isoformat()})
    
    items: List[VocabularyItem] = Field(..., description="Lista de itens de vocabul√°rio")
    total_count: int = Field(..., description="Total de palavras")
    context_relevance: float = Field(..., ge=0.0, le=1.0, description="Relev√¢ncia contextual")
    
    # NOVOS CAMPOS PARA RAG
    new_words_count: int = Field(0, description="Palavras totalmente novas")
    reinforcement_words_count: int = Field(0, description="Palavras de refor√ßo")
    rag_context_used: Dict[str, Any] = Field(default={}, description="Contexto RAG utilizado")
    progression_level: str = Field(default="intermediate", description="N√≠vel de progress√£o")
    
    # NOVOS CAMPOS PARA IPA
    phoneme_coverage: Dict[str, int] = Field(default={}, description="Cobertura de fonemas IPA")
    pronunciation_variants: List[str] = Field(default=[], description="Variantes de pron√∫ncia utilizadas")
    phonetic_complexity: str = Field(default="medium", description="Complexidade fon√©tica geral")
    
    generated_at: datetime = Field(default_factory=datetime.now)
    
    @field_validator('total_count')
    @classmethod
    def validate_total_count(cls, v: int, info: ValidationInfo) -> int:
        """Validar contagem total."""
        if hasattr(info, 'data') and 'items' in info.data:
            items = info.data['items']
            if v != len(items):
                return len(items)
        return v
    
    @field_validator('items')
    @classmethod
    def validate_items_not_empty(cls, v: List[VocabularyItem]) -> List[VocabularyItem]:
        """Validar que h√° pelo menos alguns itens."""
        if len(v) == 0:
            raise ValueError("Se√ß√£o de vocabul√°rio deve ter pelo menos 1 item")
        
        if len(v) > 50:
            raise ValueError("Se√ß√£o de vocabul√°rio n√£o deve ter mais de 50 itens")
        
        return v
    
    @field_validator('phonetic_complexity')
    @classmethod
    def validate_phonetic_complexity(cls, v: str) -> str:
        """Validar complexidade fon√©tica."""
        valid_complexities = {"simple", "medium", "complex", "very_complex"}
        
        if v.lower() not in valid_complexities:
            raise ValueError(f"Complexidade fon√©tica deve ser uma de: {', '.join(valid_complexities)}")
        
        return v.lower()


# =============================================================================
# CONTENT MODELS (Tips & Grammar) - ATUALIZADOS PYDANTIC V2
# =============================================================================

class TipsContent(BaseModel):
    """Conte√∫do de TIPS para unidades lexicais."""
    strategy: TipStrategy = Field(..., description="Estrat√©gia TIPS aplicada")
    title: str = Field(..., description="T√≠tulo da estrat√©gia")
    explanation: str = Field(..., description="Explica√ß√£o da estrat√©gia")
    examples: List[str] = Field(..., description="Exemplos pr√°ticos")
    practice_suggestions: List[str] = Field(..., description="Sugest√µes de pr√°tica")
    memory_techniques: List[str] = Field(..., description="T√©cnicas de memoriza√ß√£o")
    
    # NOVOS CAMPOS PARA RAG
    vocabulary_coverage: List[str] = Field(default=[], description="Vocabul√°rio coberto pela estrat√©gia")
    complementary_strategies: List[str] = Field(default=[], description="Estrat√©gias complementares sugeridas")
    selection_rationale: str = Field(default="", description="Por que esta estrat√©gia foi selecionada")
    
    # NOVOS CAMPOS PARA FONEMAS
    phonetic_focus: List[str] = Field(default=[], description="Fonemas ou padr√µes fon√©ticos focalizados")
    pronunciation_tips: List[str] = Field(default=[], description="Dicas espec√≠ficas de pron√∫ncia")


class GrammarContent(BaseModel):
    """Conte√∫do de GRAMMAR para unidades gramaticais."""
    strategy: GrammarStrategy = Field(..., description="Estrat√©gia GRAMMAR aplicada")
    grammar_point: str = Field(..., description="Ponto gramatical principal")
    systematic_explanation: str = Field(..., description="Explica√ß√£o sistem√°tica")
    usage_rules: List[str] = Field(..., description="Regras de uso")
    examples: List[str] = Field(..., description="Exemplos contextualizados")
    l1_interference_notes: List[str] = Field(..., description="Notas sobre interfer√™ncia L1")
    common_mistakes: List[Dict[str, str]] = Field(..., description="Erros comuns e corre√ß√µes")
    
    # NOVOS CAMPOS PARA RAG
    vocabulary_integration: List[str] = Field(default=[], description="Como integra com o vocabul√°rio")
    previous_grammar_connections: List[str] = Field(default=[], description="Conex√µes com gram√°tica anterior")
    selection_rationale: str = Field(default="", description="Por que esta estrat√©gia foi selecionada")


# =============================================================================
# ASSESSMENT MODELS - ATUALIZADOS COM BALANCEAMENTO PYDANTIC V2
# =============================================================================

class AssessmentActivity(BaseModel):
    """Atividade de avalia√ß√£o."""
    type: AssessmentType = Field(..., description="Tipo de atividade")
    title: str = Field(..., description="T√≠tulo da atividade")
    instructions: str = Field(..., description="Instru√ß√µes da atividade")
    content: Dict[str, Any] = Field(..., description="Conte√∫do espec√≠fico da atividade")
    answer_key: Dict[str, Any] = Field(..., description="Gabarito da atividade")
    estimated_time: int = Field(..., description="Tempo estimado em minutos")
    
    # NOVOS CAMPOS PARA BALANCEAMENTO
    difficulty_level: str = Field(default="intermediate", description="N√≠vel de dificuldade")
    skills_assessed: List[str] = Field(default=[], description="Habilidades avaliadas")
    vocabulary_focus: List[str] = Field(default=[], description="Vocabul√°rio focado")
    
    # NOVOS CAMPOS PARA FONEMAS
    pronunciation_focus: bool = Field(False, description="Atividade foca em pron√∫ncia")
    phonetic_elements: List[str] = Field(default=[], description="Elementos fon√©ticos avaliados")
    
    model_config = {
        "json_schema_extra": {
            "example": {
                "type": "gap_fill",
                "title": "Complete the sentences",
                "instructions": "Fill in the blanks with the appropriate words from the vocabulary.",
                "content": {
                    "sentences": [
                        "I need to make a _______ for dinner.",
                        "The hotel has excellent _______."
                    ],
                    "word_bank": ["reservation", "service"]
                },
                "answer_key": {
                    "1": "reservation",
                    "2": "service"
                },
                "estimated_time": 10,
                "difficulty_level": "intermediate",
                "skills_assessed": ["vocabulary_recognition", "context_application"],
                "vocabulary_focus": ["reservation", "service"],
                "pronunciation_focus": False,
                "phonetic_elements": []
            }
        }
    }


class AssessmentSection(BaseModel):
    """Se√ß√£o completa de avalia√ß√£o - ATUALIZADA COM BALANCEAMENTO."""
    activities: List[AssessmentActivity] = Field(..., description="Lista de atividades (2 selecionadas)")
    selection_rationale: str = Field(..., description="Justificativa da sele√ß√£o")
    total_estimated_time: int = Field(..., description="Tempo total estimado")
    skills_assessed: List[str] = Field(..., description="Habilidades avaliadas")
    
    # NOVOS CAMPOS PARA BALANCEAMENTO
    balance_analysis: Dict[str, Any] = Field(default={}, description="An√°lise de balanceamento")
    underused_activities: List[str] = Field(default=[], description="Atividades subutilizadas")
    complementary_pair: bool = Field(True, description="S√£o atividades complementares?")


# =============================================================================
# COMMON MISTAKE MODEL - CLASSE FALTANTE QUE ESTAVA CAUSANDO OS ERROS
# =============================================================================

class CommonMistake(BaseModel):
    """Modelo para erros comuns identificados e suas corre√ß√µes - Pydantic V2 Otimizada."""
    mistake_type: str = Field(..., description="Tipo de erro comum")
    incorrect_form: str = Field(..., description="Forma incorreta")
    correct_form: str = Field(..., description="Forma correta")
    explanation: str = Field(..., description="Explica√ß√£o do erro")
    examples: List[str] = Field(default=[], description="Exemplos do erro")
    frequency: str = Field(default="medium", description="Frequ√™ncia do erro")
    cefr_level: str = Field(default="A2", description="N√≠vel CEFR onde o erro ocorre")
    
    # Campos espec√≠ficos para brasileiros
    l1_interference: bool = Field(False, description="√â interfer√™ncia do portugu√™s?")
    prevention_strategy: str = Field(default="explicit_instruction", description="Estrat√©gia de preven√ß√£o")
    related_grammar_point: Optional[str] = Field(None, description="Ponto gramatical relacionado")
    
    # ‚úÖ MELHORIA 1: Campos adicionais √∫teis
    context_where_occurs: Optional[str] = Field(None, description="Contexto onde o erro √© comum")
    age_group_frequency: Optional[str] = Field(None, description="Faixa et√°ria onde √© mais comum")
    remedial_exercises: List[str] = Field(default=[], description="Exerc√≠cios espec√≠ficos para corre√ß√£o")
    
    # ‚úÖ MELHORIA 2: Metadados de tracking
    first_observed: Optional[datetime] = Field(None, description="Quando foi observado primeiro")
    severity_score: Optional[float] = Field(None, ge=0.0, le=1.0, description="Score de severidade (0-1)")
    
    @field_validator('mistake_type')
    @classmethod
    def validate_mistake_type(cls, v: str) -> str:
        """Validar tipo de erro."""
        valid_types = {
            "grammatical", "lexical", "phonetic", "semantic", 
            "syntactic", "spelling", "pronunciation", "usage",
            # ‚úÖ MELHORIA 3: Tipos adicionais espec√≠ficos para brasileiros
            "article_omission", "preposition_confusion", "false_friend",
            "word_order", "verb_tense", "modal_usage"
        }
        
        if v.lower() not in valid_types:
            raise ValueError(f"Tipo de erro deve ser um de: {', '.join(sorted(valid_types))}")
        
        return v.lower()
    
    @field_validator('frequency')
    @classmethod
    def validate_frequency(cls, v: str) -> str:
        """Validar frequ√™ncia do erro."""
        valid_frequencies = {"very_low", "low", "medium", "high", "very_high"}
        
        if v.lower() not in valid_frequencies:
            raise ValueError(f"Frequ√™ncia deve ser uma de: {', '.join(sorted(valid_frequencies))}")
        
        return v.lower()
    
    @field_validator('prevention_strategy')
    @classmethod
    def validate_prevention_strategy(cls, v: str) -> str:
        """Validar estrat√©gia de preven√ß√£o."""
        valid_strategies = {
            "explicit_instruction", "contrastive_exercises", "drilling", 
            "error_correction", "awareness_raising", "input_enhancement",
            "consciousness_raising", "form_focused_instruction",
            # ‚úÖ MELHORIA 4: Estrat√©gias adicionais espec√≠ficas
            "pattern_recognition", "metalinguistic_awareness", 
            "controlled_practice", "communicative_practice"
        }
        
        if v.lower() not in valid_strategies:
            raise ValueError(f"Estrat√©gia deve ser uma de: {', '.join(sorted(valid_strategies))}")
        
        return v.lower()
    
    # ‚úÖ MELHORIA 5: Validador para CEFR
    @field_validator('cefr_level')
    @classmethod
    def validate_cefr_level(cls, v: str) -> str:
        """Validar n√≠vel CEFR."""
        valid_levels = {"A1", "A2", "B1", "B2", "C1", "C2"}
        
        if v.upper() not in valid_levels:
            raise ValueError(f"N√≠vel CEFR deve ser um de: {', '.join(sorted(valid_levels))}")
        
        return v.upper()
    
    # ‚úÖ MELHORIA 6: Validador para age_group_frequency
    @field_validator('age_group_frequency')
    @classmethod
    def validate_age_group(cls, v: Optional[str]) -> Optional[str]:
        """Validar faixa et√°ria."""
        if v is None:
            return v
            
        valid_groups = {"children", "teenagers", "young_adults", "adults", "seniors", "all_ages"}
        
        if v.lower() not in valid_groups:
            raise ValueError(f"Faixa et√°ria deve ser uma de: {', '.join(sorted(valid_groups))}")
        
        return v.lower()
    
    model_config = {
        "json_schema_extra": {
            "example": {
                "mistake_type": "grammatical",
                "incorrect_form": "I have 25 years",
                "correct_form": "I am 25 years old",
                "explanation": "Portuguese speakers often use 'have' for age due to L1 interference",
                "examples": [
                    "She has 30 years ‚Üí She is 30 years old",
                    "How many years do you have? ‚Üí How old are you?"
                ],
                "frequency": "very_high",
                "cefr_level": "A1",
                "l1_interference": True,
                "prevention_strategy": "contrastive_exercises",
                "related_grammar_point": "be_vs_have",
                "context_where_occurs": "Personal introductions, age discussions",
                "age_group_frequency": "all_ages",
                "remedial_exercises": [
                    "Age expression drills",
                    "Contrastive PT vs EN exercises",
                    "Controlled practice with BE + age"
                ],
                "severity_score": 0.9
            }
        }
    }


class CommonMistakeSection(BaseModel):
    """Se√ß√£o de erros comuns para uma unidade - Pydantic V2."""
    mistakes: List[CommonMistake] = Field(..., description="Lista de erros comuns")
    total_mistakes: int = Field(..., description="Total de erros identificados")
    l1_interference_count: int = Field(default=0, description="Quantos s√£o interfer√™ncia L1")
    prevention_strategies: List[str] = Field(default=[], description="Estrat√©gias de preven√ß√£o")
    difficulty_level: str = Field(default="intermediate", description="N√≠vel de dificuldade geral")
    
    generated_at: datetime = Field(default_factory=datetime.now)
    
    @field_validator('total_mistakes')
    @classmethod
    def validate_total_mistakes(cls, v: int, info: ValidationInfo) -> int:
        """Validar contagem total."""
        if hasattr(info, 'data') and 'mistakes' in info.data:
            mistakes = info.data['mistakes']
            if v != len(mistakes):
                return len(mistakes)
        return v
    
    @field_validator('l1_interference_count')
    @classmethod
    def validate_l1_count(cls, v: int, info: ValidationInfo) -> int:
        """Validar contagem de interfer√™ncia L1."""
        if hasattr(info, 'data') and 'mistakes' in info.data:
            mistakes = info.data['mistakes']
            actual_l1_count = sum(1 for mistake in mistakes if mistake.l1_interference)
            if v != actual_l1_count:
                return actual_l1_count
        return v
    
    model_config = {
        "json_schema_extra": {
            "example": {
                "mistakes": [
                    {
                        "mistake_type": "grammatical",
                        "incorrect_form": "I have 25 years",
                        "correct_form": "I am 25 years old",
                        "explanation": "Age expression error",
                        "l1_interference": True,
                        "prevention_strategy": "contrastive_exercises"
                    }
                ],
                "total_mistakes": 1,
                "l1_interference_count": 1,
                "prevention_strategies": ["contrastive_exercises", "explicit_instruction"],
                "difficulty_level": "beginner"
            }
        }
    }


# =============================================================================
# UNIT COMPLETE MODEL - ATUALIZADO COM HIERARQUIA PYDANTIC V2
# =============================================================================

class UnitResponse(BaseModel):
    """Response completa da unidade - ATUALIZADA COM HIERARQUIA."""
    # HIERARQUIA (novos campos obrigat√≥rios)
    id: str = Field(..., description="ID √∫nico da unidade")
    course_id: str = Field(..., description="ID do curso")
    book_id: str = Field(..., description="ID do book")
    sequence_order: int = Field(..., description="Ordem sequencial no book")
    
    # Informa√ß√µes b√°sicas
    title: str = Field(..., description="T√≠tulo da unidade")
    main_aim: str = Field(..., description="Objetivo principal")
    subsidiary_aims: List[str] = Field(..., description="Objetivos subsidi√°rios")
    
    # Metadata
    unit_type: UnitType = Field(..., description="Tipo de unidade")
    cefr_level: CEFRLevel = Field(..., description="N√≠vel CEFR")
    language_variant: LanguageVariant = Field(..., description="Variante do idioma")
    status: UnitStatus = Field(..., description="Status atual")
    
    # Content Sections
    images: List["ImageInfo"] = Field(default=[], description="Informa√ß√µes das imagens")
    vocabulary: Optional[VocabularySection] = Field(None, description="Se√ß√£o de vocabul√°rio")
    sentences: Optional["SentencesSection"] = Field(None, description="Se√ß√£o de sentences")
    tips: Optional[TipsContent] = Field(None, description="Conte√∫do TIPS (se lexical)")
    grammar: Optional[GrammarContent] = Field(None, description="Conte√∫do GRAMMAR (se grammar)")
    qa: Optional["QASection"] = Field(None, description="Se√ß√£o Q&A")
    assessments: Optional[AssessmentSection] = Field(None, description="Se√ß√£o de avalia√ß√£o")
    
    # PROGRESS√ÉO PEDAG√ìGICA (novos campos)
    strategies_used: List[str] = Field(default=[], description="Estrat√©gias j√° usadas")
    assessments_used: List[str] = Field(default=[], description="Tipos de atividades j√° usadas")
    vocabulary_taught: List[str] = Field(default=[], description="Vocabul√°rio ensinado nesta unidade")
    
    # NOVOS CAMPOS PARA FONEMAS
    phonemes_introduced: List[str] = Field(default=[], description="Fonemas introduzidos nesta unidade")
    pronunciation_focus: Optional[str] = Field(None, description="Foco de pron√∫ncia da unidade")
    
    # CONTEXTO HIER√ÅRQUICO (informa√ß√µes derivadas)
    hierarchy_info: Optional[Dict[str, Any]] = Field(None, description="Informa√ß√µes da hierarquia")
    progression_analysis: Optional[Dict[str, Any]] = Field(None, description="An√°lise de progress√£o")
    
    # Timestamps
    created_at: datetime = Field(default_factory=datetime.now)
    updated_at: datetime = Field(default_factory=datetime.now)
    
    # Quality Control
    quality_score: Optional[float] = Field(None, ge=0.0, le=1.0, description="Score de qualidade")
    checklist_completed: List[str] = Field(default=[], description="Checklist de qualidade completado")

    model_config = {
        "json_schema_extra": {
            "example": {
                "id": "unit_hotel_reservations_001",
                "course_id": "course_english_beginners",
                "book_id": "book_foundation_a1",
                "sequence_order": 5,
                "title": "Hotel Reservations",
                "main_aim": "Students will be able to make hotel reservations using appropriate vocabulary and phrases",
                "subsidiary_aims": [
                    "Use reservation-related vocabulary accurately",
                    "Apply polite language in formal situations",
                    "Understand hotel policies and procedures"
                ],
                "unit_type": "lexical_unit",
                "cefr_level": "A2",
                "language_variant": "american_english",
                "status": "completed",
                "strategies_used": ["collocations", "chunks"],
                "assessments_used": ["gap_fill", "matching"],
                "vocabulary_taught": ["reservation", "check-in", "availability", "suite"],
                "phonemes_introduced": ["/Àårez…ôrÀàve…™ É…ôn/", "/Àà ß…õk …™n/"],
                "pronunciation_focus": "stress_patterns",
                "quality_score": 0.92
            }
        }
    }


# =============================================================================
# ADDITIONAL MODELS FOR SENTENCES AND QA - PYDANTIC V2
# =============================================================================

class Sentence(BaseModel):
    """Sentence conectada ao vocabul√°rio."""
    text: str = Field(..., description="Texto da sentence")
    vocabulary_used: List[str] = Field(..., description="Palavras do vocabul√°rio utilizadas")
    context_situation: str = Field(..., description="Situa√ß√£o contextual")
    complexity_level: str = Field(..., description="N√≠vel de complexidade")
    
    # NOVOS CAMPOS PARA PROGRESS√ÉO
    reinforces_previous: List[str] = Field(default=[], description="Vocabul√°rio anterior refor√ßado")
    introduces_new: List[str] = Field(default=[], description="Novo vocabul√°rio introduzido")
    
    # NOVOS CAMPOS PARA FONEMAS
    phonetic_features: List[str] = Field(default=[], description="Caracter√≠sticas fon√©ticas destacadas")
    pronunciation_notes: Optional[str] = Field(None, description="Notas de pron√∫ncia")
    
    model_config = {
        "json_schema_extra": {
            "example": {
                "text": "I need to make a reservation for two people tonight.",
                "vocabulary_used": ["reservation"],
                "context_situation": "restaurant_booking",
                "complexity_level": "intermediate",
                "reinforces_previous": [],
                "introduces_new": ["reservation"],
                "phonetic_features": ["word_stress", "schwa_reduction"],
                "pronunciation_notes": "Note the stress on 'reser-VA-tion'"
            }
        }
    }


class SentencesSection(BaseModel):
    """Se√ß√£o de sentences."""
    sentences: List[Sentence] = Field(..., description="Lista de sentences")
    vocabulary_coverage: float = Field(..., ge=0.0, le=1.0, description="Cobertura do vocabul√°rio")
    
    # NOVOS CAMPOS PARA RAG
    contextual_coherence: float = Field(default=0.8, description="Coer√™ncia contextual")
    progression_appropriateness: float = Field(default=0.8, description="Adequa√ß√£o √† progress√£o")
    
    # NOVOS CAMPOS PARA FONEMAS
    phonetic_progression: List[str] = Field(default=[], description="Progress√£o fon√©tica nas sentences")
    pronunciation_patterns: List[str] = Field(default=[], description="Padr√µes de pron√∫ncia abordados")
    
    generated_at: datetime = Field(default_factory=datetime.now)


class QASection(BaseModel):
    """Se√ß√£o de perguntas e respostas."""
    questions: List[str] = Field(..., description="Perguntas para estudantes")
    answers: List[str] = Field(..., description="Respostas completas (para professores)")
    pedagogical_notes: List[str] = Field(..., description="Notas pedag√≥gicas")
    difficulty_progression: str = Field(..., description="Progress√£o de dificuldade")
    
    # NOVOS CAMPOS PARA CONTEXTO
    vocabulary_integration: List[str] = Field(default=[], description="Vocabul√°rio integrado")
    cognitive_levels: List[str] = Field(default=[], description="N√≠veis cognitivos das perguntas")
    
    # NOVOS CAMPOS PARA FONEMAS
    pronunciation_questions: List[str] = Field(default=[], description="Perguntas sobre pron√∫ncia")
    phonetic_awareness: List[str] = Field(default=[], description="Consci√™ncia fon√©tica desenvolvida")


class ImageInfo(BaseModel):
    """Informa√ß√µes da imagem processada."""
    filename: str = Field(..., description="Nome do arquivo")
    description: str = Field(..., description="Descri√ß√£o da imagem pela IA")
    objects_detected: List[str] = Field(..., description="Objetos detectados")
    text_detected: Optional[str] = Field(None, description="Texto detectado na imagem")
    relevance_score: float = Field(..., ge=0.0, le=1.0, description="Score de relev√¢ncia")
    
    # NOVOS CAMPOS PARA PROGRESS√ÉO
    vocabulary_suggestions: List[str] = Field(default=[], description="Vocabul√°rio sugerido pela imagem")
    context_themes: List[str] = Field(default=[], description="Temas contextuais identificados")


# =============================================================================
# PROGRESS & STATUS MODELS - ATUALIZADOS PYDANTIC V2
# =============================================================================

class GenerationProgress(BaseModel):
    """Progresso da gera√ß√£o de conte√∫do."""
    unit_id: str = Field(..., description="ID da unidade")
    course_id: str = Field(..., description="ID do curso")
    book_id: str = Field(..., description="ID do book")
    sequence_order: int = Field(..., description="Sequ√™ncia no book")
    
    current_step: str = Field(..., description="Etapa atual")
    progress_percentage: int = Field(..., ge=0, le=100, description="Porcentagem de progresso")
    message: str = Field(..., description="Mensagem de status")
    estimated_remaining_time: Optional[int] = Field(None, description="Tempo estimado restante (segundos)")
    
    # NOVOS CAMPOS PARA CONTEXTO RAG
    rag_context_loaded: bool = Field(False, description="Contexto RAG carregado?")
    precedent_units_found: int = Field(0, description="Unidades precedentes encontradas")
    vocabulary_overlap_analysis: Optional[Dict[str, Any]] = Field(None, description="An√°lise de sobreposi√ß√£o")
    
    # NOVOS CAMPOS PARA FONEMAS
    phoneme_analysis_completed: bool = Field(False, description="An√°lise fon√©tica completada?")
    pronunciation_validation: Optional[Dict[str, Any]] = Field(None, description="Valida√ß√£o de pron√∫ncia")
    
    details: Optional[Dict[str, Any]] = Field(None, description="Detalhes adicionais")


class ErrorResponse(BaseModel):
    """Response de erro padronizado."""
    success: bool = Field(False, description="Sempre False para erros")
    error_code: str = Field(..., description="C√≥digo do erro")
    message: str = Field(..., description="Mensagem de erro")
    details: Optional[Dict[str, Any]] = Field(None, description="Detalhes do erro")
    timestamp: datetime = Field(default_factory=datetime.now)
    
    # NOVOS CAMPOS PARA DEPURA√á√ÉO
    hierarchy_context: Optional[Dict[str, Any]] = Field(None, description="Contexto hier√°rquico do erro")
    suggested_fixes: List[str] = Field(default=[], description="Sugest√µes de corre√ß√£o")


class SuccessResponse(BaseModel):
    """Response de sucesso padronizado."""
    model_config = ConfigDict(json_encoders={datetime: lambda v: v.isoformat()})
    
    success: bool = Field(True, description="Sempre True para sucesso")
    data: Dict[str, Any] = Field(..., description="Dados da resposta")
    message: Optional[str] = Field(None, description="Mensagem opcional")
    timestamp: datetime = Field(default_factory=datetime.now)
    
    # NOVOS CAMPOS PARA CONTEXTO
    hierarchy_info: Optional[Dict[str, Any]] = Field(None, description="Informa√ß√µes hier√°rquicas")
    next_suggested_actions: List[str] = Field(default=[], description="Pr√≥ximas a√ß√µes sugeridas")
    
    def __init__(self, **data):
        import logging
        logger = logging.getLogger(__name__)
        logger.info("üîç [DEBUG] Criando SuccessResponse...")
        super().__init__(**data)
        logger.info("üîç [DEBUG] SuccessResponse criado com sucesso")


# =============================================================================
# BATCH AND BULK OPERATION MODELS
# =============================================================================

class BulkUnitStatus(BaseModel):
    """Status de opera√ß√£o em lote."""
    total_units: int = Field(..., description="Total de unidades processadas")
    successful: int = Field(..., description="Unidades processadas com sucesso")
    failed: int = Field(..., description="Unidades que falharam")
    errors: List[Dict[str, str]] = Field(default=[], description="Detalhes dos erros")
    processing_time: float = Field(..., description="Tempo total de processamento")


class CourseStatistics(BaseModel):
    """Estat√≠sticas do curso."""
    course_id: str
    course_name: str
    total_books: int
    total_units: int
    completed_units: int
    average_quality_score: float
    
    # DISTRIBUI√á√ïES
    units_by_level: Dict[str, int] = Field(default={}, description="Unidades por n√≠vel CEFR")
    units_by_type: Dict[str, int] = Field(default={}, description="Unidades por tipo")
    strategy_distribution: Dict[str, int] = Field(default={}, description="Distribui√ß√£o de estrat√©gias")
    assessment_distribution: Dict[str, int] = Field(default={}, description="Distribui√ß√£o de atividades")
    
    # PROGRESS√ÉO
    vocabulary_progression: Dict[str, Any] = Field(default={}, description="Progress√£o de vocabul√°rio")
    quality_progression: List[float] = Field(default=[], description="Progress√£o da qualidade")
    
    # NOVOS CAMPOS PARA FONEMAS
    phoneme_distribution: Dict[str, int] = Field(default={}, description="Distribui√ß√£o de fonemas IPA")
    pronunciation_variants: List[str] = Field(default=[], description="Variantes de pron√∫ncia usadas")
    phonetic_complexity_trend: List[str] = Field(default=[], description="Tend√™ncia de complexidade fon√©tica")
    
    last_updated: datetime = Field(default_factory=datetime.now)


# =============================================================================
# VOCABULARY GENERATION MODELS - NOVOS PARA PROMPT 6 PYDANTIC V2
# =============================================================================

# =============================================================================
# SOLVE ASSESSMENTS MODELS - CORRE√á√ÉO IA DE ATIVIDADES
# =============================================================================

class ItemCorrection(BaseModel):
    """Corre√ß√£o individual de item de assessment."""
    item_id: str = Field(..., description="ID do item/quest√£o")
    student_answer: str = Field(..., description="Resposta do aluno")
    correct_answer: str = Field(..., description="Resposta esperada")
    result: str = Field(..., description="Resultado: correct, incorrect, partially_correct")
    score_earned: int = Field(..., ge=0, description="Pontos obtidos")
    score_total: int = Field(..., ge=1, description="Pontos poss√≠veis")
    feedback: str = Field(..., description="Feedback espec√≠fico para este item")
    l1_interference: Optional[str] = Field(None, description="Padr√£o de interfer√™ncia PT‚ÜíEN identificado")

class ErrorAnalysis(BaseModel):
    """An√°lise detalhada de erros."""
    most_common_errors: List[str] = Field(default=[], description="Tipos de erro mais frequentes")
    l1_interference_patterns: List[str] = Field(default=[], description="Padr√µes espec√≠ficos PT‚ÜíEN")
    recurring_mistakes: List[str] = Field(default=[], description="Erros que se repetem")
    error_frequency: Dict[str, int] = Field(default={}, description="Frequ√™ncia de cada tipo de erro")

class ConstructiveFeedback(BaseModel):
    """Feedback construtivo para o aluno."""
    strengths_demonstrated: List[str] = Field(default=[], description="Aspectos positivos identificados")
    areas_for_improvement: List[str] = Field(default=[], description="√Åreas espec√≠ficas para melhoria")
    study_recommendations: List[str] = Field(default=[], description="Sugest√µes de estudo direcionadas")
    next_steps: List[str] = Field(default=[], description="Pr√≥ximos passos no aprendizado")

class PedagogicalNotes(BaseModel):
    """Notas pedag√≥gicas para professores."""
    class_performance_patterns: List[str] = Field(default=[], description="Padr√µes observados na turma")
    remedial_activities: List[str] = Field(default=[], description="Atividades remediais sugeridas")
    differentiation_needed: List[str] = Field(default=[], description="Adapta√ß√µes necess√°rias")
    followup_assessments: List[str] = Field(default=[], description="Avalia√ß√µes de acompanhamento")

class SolveAssessmentResult(BaseModel):
    """Resultado completo da corre√ß√£o de assessment pela IA."""
    model_config = ConfigDict(json_encoders={datetime: lambda v: v.isoformat()})
    
    # === AVALIA√á√ÉO GERAL ===
    total_score: int = Field(..., description="Pontua√ß√£o total obtida")
    total_possible: int = Field(..., description="Pontua√ß√£o total poss√≠vel")
    performance_level: str = Field(..., description="N√≠vel de desempenho: excellent, good, satisfactory, needs_improvement")
    cefr_demonstration: str = Field(..., description="N√≠vel CEFR demonstrado: above, at, below")
    
    # === CORRE√á√ÉO ITEM POR ITEM ===
    item_corrections: List[ItemCorrection] = Field(..., description="Corre√ß√µes individuais")
    
    # === AN√ÅLISES ===
    error_analysis: ErrorAnalysis = Field(..., description="An√°lise detalhada de erros")
    constructive_feedback: ConstructiveFeedback = Field(..., description="Feedback construtivo")
    pedagogical_notes: PedagogicalNotes = Field(..., description="Notas para professores")
    
    # === METADADOS ===
    assessment_type: str = Field(..., description="Tipo de assessment corrigido")
    assessment_title: str = Field(..., description="T√≠tulo da atividade")
    unit_context: Dict[str, Any] = Field(default={}, description="Contexto da unidade")
    correction_timestamp: datetime = Field(default_factory=datetime.now)
    
    # === ESTAT√çSTICAS ===
    accuracy_percentage: float = Field(..., ge=0.0, le=100.0, description="Porcentagem de acerto")
    completion_time: Optional[float] = Field(None, description="Tempo de corre√ß√£o em segundos")
    ai_model_used: str = Field(default="gpt-5", description="Modelo IA usado na corre√ß√£o")

class SimpleSolveRequest(BaseModel):
    """Request SIMPLIFICADO para corre√ß√£o de assessment - s√≥ o essencial."""
    model_config = ConfigDict(str_strip_whitespace=True, validate_assignment=True)
    
    # === APENAS O ESSENCIAL ===
    assessment_type: str = Field(..., description="Tipo de assessment para corrigir")
    include_student_answers: bool = Field(default=False, description="Se True, usa respostas salvas no banco")
    student_context: Optional[str] = Field(None, description="Contexto adicional do estudante")


# =============================================================================
# ASSESSMENT SOLUTION MODELS - GERADOR DE GABARITOS
# =============================================================================

class AssessmentItem(BaseModel):
    """Item individual de um assessment com solu√ß√£o."""
    item_id: str = Field(..., description="ID √∫nico do item")
    question_text: str = Field(..., description="Enunciado completo da quest√£o")
    correct_answer: str = Field(..., description="Resposta correta/gabarito")
    explanation: str = Field(..., description="Explica√ß√£o detalhada da solu√ß√£o")
    difficulty_level: str = Field(..., description="N√≠vel de dificuldade: easy, medium, hard")
    skills_tested: List[str] = Field(default=[], description="Habilidades testadas")
    
class AssessmentSolution(BaseModel):
    """Solu√ß√£o completa de um assessment - NOVO MODELO PARA GABARITOS."""
    model_config = ConfigDict(json_encoders={datetime: lambda v: v.isoformat()})
    
    # === IDENTIFICA√á√ÉO ===
    assessment_type: str = Field(..., description="Tipo de assessment")
    assessment_title: str = Field(..., description="T√≠tulo da atividade")
    total_items: int = Field(..., description="Total de quest√µes")
    
    # === INSTRU√á√ïES E CONTEXTO ===
    instructions: str = Field(..., description="Instru√ß√µes completas do assessment")
    unit_context: str = Field(..., description="Contexto da unidade para refer√™ncia")
    
    # === SOLU√á√ïES ITEM POR ITEM ===
    items: List[AssessmentItem] = Field(..., description="Solu√ß√µes detalhadas de cada item")
    
    # === RESUMO PEDAG√ìGICO ===
    skills_overview: List[str] = Field(default=[], description="Vis√£o geral das habilidades testadas")
    difficulty_distribution: Dict[str, int] = Field(default={}, description="Distribui√ß√£o por dificuldade")
    teaching_notes: List[str] = Field(default=[], description="Notas para professores")
    
    # === METADADOS ===
    solution_timestamp: datetime = Field(default_factory=datetime.now)
    ai_model_used: str = Field(default="gpt-5", description="Modelo usado para gerar gabarito")
    processing_time: Optional[float] = Field(None, description="Tempo de processamento")

class SimpleGabaritoRequest(BaseModel):
    """Request para gera√ß√£o de gabarito."""
    model_config = ConfigDict(str_strip_whitespace=True)
    
    assessment_type: str = Field(..., description="Tipo de assessment para resolver")
    include_explanations: bool = Field(default=True, description="Incluir explica√ß√µes detalhadas")
    difficulty_analysis: bool = Field(default=True, description="Incluir an√°lise de dificuldade")

# =============================================================================
# CONTENT GENERATION REQUEST MODELS - PRODU√á√ÉO-READY COM VALIDA√á√ÉO ESPEC√çFICA
# =============================================================================

class VocabularyGenerationRequest(BaseModel):
    """Request espec√≠fico para gera√ß√£o de vocabul√°rio."""
    # Webhook para processamento ass√≠ncrono (OPCIONAL)
    webhook_url: Optional[str] = Field(None, description="URL para webhook - se fornecida, processar√° ass√≠ncronamente")
    
    # Configura√ß√µes b√°sicas - OBRIGAT√ìRIAS
    target_count: int = Field(..., ge=5, le=50, description="N√∫mero EXATO de palavras a gerar (obrigat√≥rio)")
    difficulty_level: Optional[str] = Field("intermediate", description="N√≠vel: beginner, intermediate, advanced")
    focus_areas: Optional[List[str]] = Field(None, description="√Åreas de foco: verbs, nouns, adjectives, etc")
    
    # Compatibilidade (deprecated)
    word_count: Optional[int] = Field(None, description="DEPRECATED: Use target_count")
    
    # Configura√ß√µes IPA
    ipa_variant: Optional[str] = Field("general_american", description="Variante IPA")
    include_alternative_pronunciations: Optional[bool] = Field(False, description="Incluir pron√∫ncias alternativas")
    phonetic_complexity: Optional[str] = Field("medium", description="Complexidade fon√©tica: simple, medium, complex")
    
    # Contexto RAG
    avoid_repetition: Optional[bool] = Field(True, description="Evitar repeti√ß√µes")
    is_revision_unit: bool = Field(False, description="Unidade de revis√£o (permite busca RAG ampla em outros books)")
    avoid_vocabulary: Optional[List[str]] = Field(None, description="Palavras a evitar")
    reinforce_vocabulary: Optional[List[str]] = Field(None, description="Palavras para refor√ßar")
    use_rag_context: Optional[bool] = Field(True, description="Usar contexto RAG")


class SentenceGenerationRequest(BaseModel):
    """Request espec√≠fico para gera√ß√£o de sentences."""
    # Webhook para processamento ass√≠ncrono (OPCIONAL)
    webhook_url: Optional[str] = Field(None, description="URL para webhook - se fornecida, processar√° ass√≠ncronamente")
    
    # Configura√ß√µes b√°sicas - OBRIGAT√ìRIAS
    target_count: int = Field(..., ge=3, le=20, description="N√∫mero EXATO de senten√ßas a gerar (obrigat√≥rio)")
    complexity_level: Optional[str] = Field("simple", description="Complexidade: simple, intermediate, complex")
    
    # Compatibilidade (deprecated)
    sentence_count: Optional[int] = Field(None, description="DEPRECATED: Use target_count")
    
    # Conex√£o com vocabul√°rio
    connect_to_vocabulary: Optional[bool] = Field(True, description="Conectar ao vocabul√°rio da unit")
    vocabulary_coverage: Optional[float] = Field(0.7, ge=0.1, le=1.0, description="% do vocabul√°rio a usar")
    
    # Contexto e variedade
    sentence_types: Optional[List[str]] = Field(None, description="Tipos: declarative, interrogative, imperative")
    avoid_repetition: Optional[bool] = Field(True, description="Evitar repeti√ß√µes")
    use_rag_context: Optional[bool] = Field(True, description="Usar contexto RAG")
    is_revision_unit: bool = Field(False, description="Unidade de revis√£o (permite busca RAG ampla em outros books)")


class TipsGenerationRequest(BaseModel):
    """Request espec√≠fico para gera√ß√£o de TIPS strategies."""
    # Webhook para processamento ass√≠ncrono (OPCIONAL)
    webhook_url: Optional[str] = Field(None, description="URL para webhook - se fornecida, processar√° ass√≠ncronamente")
    
    # Configura√ß√µes b√°sicas
    strategy_count: Optional[int] = Field(3, ge=1, le=6, description="N√∫mero de estrat√©gias")
    focus_type: Optional[str] = Field("vocabulary", description="Foco: vocabulary, pronunciation, usage")
    
    # Estrat√©gias espec√≠ficas
    preferred_strategies: Optional[List[str]] = Field(None, description="Estrat√©gias preferidas")
    avoid_strategies: Optional[List[str]] = Field(None, description="Estrat√©gias a evitar")
    
    # Contexto
    difficulty_adaptation: Optional[bool] = Field(True, description="Adaptar √† dificuldade da unit")
    use_rag_context: Optional[bool] = Field(True, description="Usar contexto RAG")
    include_l1_warnings: Optional[bool] = Field(True, description="Incluir avisos sobre interfer√™ncia L1")


class AssessmentGenerationRequest(BaseModel):
    """Request espec√≠fico para gera√ß√£o de assessments."""
    # Webhook para processamento ass√≠ncrono (OPCIONAL)
    webhook_url: Optional[str] = Field(None, description="URL para webhook - se fornecida, processar√° ass√≠ncronamente")
    
    # Configura√ß√µes b√°sicas
    assessment_count: Optional[int] = Field(2, ge=1, le=5, description="N√∫mero de atividades")
    difficulty_distribution: Optional[str] = Field("balanced", description="Distribui√ß√£o: easy, balanced, challenging")
    
    # Tipos de assessment
    preferred_types: Optional[List[str]] = Field(None, description="Tipos preferidos de atividade")
    avoid_types: Optional[List[str]] = Field(None, description="Tipos a evitar")
    
    # Balanceamento
    ensure_variety: Optional[bool] = Field(True, description="Garantir variedade de tipos")
    connect_to_content: Optional[bool] = Field(True, description="Conectar ao conte√∫do da unit")
    use_rag_context: Optional[bool] = Field(True, description="Usar contexto RAG")


class QAGenerationRequest(BaseModel):
    """Request espec√≠fico para gera√ß√£o de Q&A."""
    # Webhook para processamento ass√≠ncrono (OPCIONAL)
    webhook_url: Optional[str] = Field(None, description="URL para webhook - se fornecida, processar√° ass√≠ncronamente")
    
    # Configura√ß√µes b√°sicas - OBRIGAT√ìRIAS
    target_count: int = Field(..., ge=2, le=15, description="N√∫mero EXATO de perguntas a gerar (obrigat√≥rio)")
    bloom_levels: Optional[List[str]] = Field(["remember", "understand"], description="N√≠veis da Taxonomia de Bloom")
    
    # Compatibilidade (deprecated)
    question_count: Optional[int] = Field(None, description="DEPRECATED: Use target_count")
    
    # Tipos de pergunta
    question_types: Optional[List[str]] = Field(None, description="Tipos: multiple_choice, open_ended, true_false")
    difficulty_progression: Optional[bool] = Field(True, description="Progress√£o de dificuldade")
    
    # Contexto
    connect_to_content: Optional[bool] = Field(True, description="Conectar ao conte√∫do da unit")
    use_rag_context: Optional[bool] = Field(True, description="Usar contexto RAG")


class GrammarGenerationRequest(BaseModel):
    """Request espec√≠fico para gera√ß√£o de grammar strategies."""
    # Webhook para processamento ass√≠ncrono (OPCIONAL)
    webhook_url: Optional[str] = Field(None, description="URL para webhook - se fornecida, processar√° ass√≠ncronamente")
    
    # Configura√ß√µes b√°sicas
    strategy_count: Optional[int] = Field(3, ge=1, le=6, description="N√∫mero de estrat√©gias gramaticais")
    grammar_focus: Optional[str] = Field("unit_based", description="Foco: unit_based, specific_point, mixed")
    
    # Estrat√©gias espec√≠ficas
    preferred_strategies: Optional[List[str]] = Field(None, description="Estrat√©gias preferidas")
    complexity_level: Optional[str] = Field("intermediate", description="Complexidade: beginner, intermediate, advanced")
    
    # Contexto
    connect_to_vocabulary: Optional[bool] = Field(True, description="Conectar ao vocabul√°rio")
    use_rag_context: Optional[bool] = Field(True, description="Usar contexto RAG")
    
    model_config = {
        "json_schema_extra": {
            "example": {
                "images_context": [
                    {
                        "description": "Hotel reception with people checking in",
                        "objects": ["desk", "receptionist", "guests", "luggage"],
                        "themes": ["hospitality", "travel", "accommodation"]
                    }
                ],
                "target_count": 25,
                "cefr_level": "A2",
                "language_variant": "american_english",
                "unit_type": "lexical_unit",
                "ipa_variant": "general_american",
                "include_alternative_pronunciations": False,
                "phonetic_complexity": "medium",
                "avoid_vocabulary": ["hello", "goodbye"],
                "reinforce_vocabulary": ["hotel", "room"]
            }
        }
    }


class VocabularyGenerationResponse(BaseModel):
    """Response da gera√ß√£o de vocabul√°rio."""
    vocabulary_section: VocabularySection = Field(..., description="Se√ß√£o de vocabul√°rio gerada")
    generation_metadata: Dict[str, Any] = Field(..., description="Metadados da gera√ß√£o")
    rag_analysis: Dict[str, Any] = Field(..., description="An√°lise RAG aplicada")
    quality_metrics: Dict[str, float] = Field(..., description="M√©tricas de qualidade")
    
    # M√âTRICAS DE IPA
    phoneme_analysis: Dict[str, Any] = Field(..., description="An√°lise dos fonemas inclu√≠dos")
    pronunciation_coverage: Dict[str, float] = Field(..., description="Cobertura de padr√µes de pron√∫ncia")
    
    model_config = {
        "json_schema_extra": {
            "example": {
                "generation_metadata": {
                    "generation_time_ms": 1500,
                    "ai_model_used": "gpt-4o-mini",
                    "mcp_analysis_included": True,
                    "rag_context_applied": True
                },
                "rag_analysis": {
                    "words_avoided": 3,
                    "words_reinforced": 2,
                    "new_words_generated": 20,
                    "progression_appropriate": True
                },
                "quality_metrics": {
                    "context_relevance": 0.92,
                    "cefr_appropriateness": 0.95,
                    "vocabulary_diversity": 0.88,
                    "phonetic_accuracy": 0.97
                },
                "phoneme_analysis": {
                    "total_unique_phonemes": 35,
                    "most_common_phonemes": ["/…ô/", "/…™/", "/e…™/"],
                    "stress_patterns": ["primary_first", "primary_second"],
                    "syllable_distribution": {"1": 5, "2": 12, "3": 6, "4+": 2}
                },
                "pronunciation_coverage": {
                    "vowel_sounds": 0.85,
                    "consonant_clusters": 0.70,
                    "stress_patterns": 0.90
                }
            }
        }
    }


# =============================================================================
# PHONETIC VALIDATION MODELS - NOVOS PYDANTIC V2
# =============================================================================

class PhoneticValidationResult(BaseModel):
    """Resultado da valida√ß√£o fon√©tica de um item de vocabul√°rio."""
    word: str = Field(..., description="Palavra validada")
    phoneme: str = Field(..., description="Fonema validado")
    is_valid: bool = Field(..., description="Se a valida√ß√£o passou")
    
    validation_details: Dict[str, Any] = Field(..., description="Detalhes da valida√ß√£o")
    suggestions: List[str] = Field(default=[], description="Sugest√µes de corre√ß√£o")
    confidence_score: float = Field(..., ge=0.0, le=1.0, description="Confian√ßa na valida√ß√£o")
    
    model_config = {
        "json_schema_extra": {
            "example": {
                "word": "restaurant",
                "phoneme": "/Ààr…õst…ôr…ënt/",
                "is_valid": True,
                "validation_details": {
                    "ipa_symbols_valid": True,
                    "stress_marking_correct": True,
                    "syllable_count_matches": True,
                    "variant_appropriate": True
                },
                "suggestions": [],
                "confidence_score": 0.98
            }
        }
    }


class BulkPhoneticValidation(BaseModel):
    """Valida√ß√£o fon√©tica em lote."""
    total_items: int = Field(..., description="Total de itens validados")
    valid_items: int = Field(..., description="Itens v√°lidos")
    invalid_items: int = Field(..., description="Itens inv√°lidos")
    
    validation_results: List[PhoneticValidationResult] = Field(..., description="Resultados individuais")
    overall_quality: float = Field(..., ge=0.0, le=1.0, description="Qualidade geral")
    
    common_errors: List[str] = Field(default=[], description="Erros comuns encontrados")
    improvement_suggestions: List[str] = Field(default=[], description="Sugest√µes de melhoria")


# =============================================================================
# MIGRATION HELPERS (Para compatibilidade) - ATUALIZADO PYDANTIC V2
# =============================================================================

class LegacyUnitAdapter(BaseModel):
    """Adaptador para unidades do sistema antigo."""
    
    @classmethod
    def from_legacy_unit(cls, legacy_data: Dict[str, Any]) -> UnitResponse:
        """Converte unidade do formato antigo para o novo com hierarquia."""
        # Implementar l√≥gica de migra√ß√£o
        # Por enquanto, valores padr√£o para hierarquia
        return UnitResponse(
            id=legacy_data.get("id", "legacy_unit"),
            course_id=legacy_data.get("course_id", "course_default"),
            book_id=legacy_data.get("book_id", "book_default"),
            sequence_order=legacy_data.get("sequence_order", 1),
            title=legacy_data.get("title", "Legacy Unit"),
            main_aim=legacy_data.get("main_aim", "Legacy main aim"),
            subsidiary_aims=legacy_data.get("subsidiary_aims", []),
            unit_type=UnitType(legacy_data.get("unit_type", "lexical_unit")),
            cefr_level=CEFRLevel(legacy_data.get("cefr_level", "A1")),
            language_variant=LanguageVariant(legacy_data.get("language_variant", "american_english")),
            status=UnitStatus(legacy_data.get("status", "creating")),
            vocabulary=legacy_data.get("vocabulary"),
            sentences=legacy_data.get("sentences"),
            tips=legacy_data.get("tips"),
            grammar=legacy_data.get("grammar"),
            qa=legacy_data.get("qa"),
            assessments=legacy_data.get("assessments"),
            # NOVOS CAMPOS PARA COMPATIBILIDADE
            phonemes_introduced=legacy_data.get("phonemes_introduced", []),
            pronunciation_focus=legacy_data.get("pronunciation_focus"),
            created_at=legacy_data.get("created_at", datetime.now()),
            updated_at=legacy_data.get("updated_at", datetime.now())
        )
    
    @classmethod
    def migrate_vocabulary_to_ipa(cls, legacy_vocabulary: List[Dict[str, Any]]) -> List[VocabularyItem]:
        """Migrar vocabul√°rio antigo para formato com IPA."""
        migrated_items = []
        
        for item in legacy_vocabulary:
            # Gerar fonema b√°sico se n√£o existir
            phoneme = item.get("phoneme")
            if not phoneme:
                # Fonema placeholder - deveria ser gerado por IA
                word = item.get("word", "")
                phoneme = f"/placeholder_{word}/"
            
            try:
                vocabulary_item = VocabularyItem(
                    word=item.get("word", ""),
                    phoneme=phoneme,
                    definition=item.get("definition", ""),
                    example=item.get("example", ""),
                    word_class=item.get("word_class", "noun"),
                    frequency_level=item.get("frequency_level", "medium"),
                    context_relevance=item.get("context_relevance", 0.5),
                    is_reinforcement=item.get("is_reinforcement", False),
                    ipa_variant="general_american",
                    syllable_count=item.get("syllable_count", 1)
                )
                migrated_items.append(vocabulary_item)
            except Exception as e:
                # Log erro e pular item inv√°lido
                print(f"Erro ao migrar item {item.get('word', 'unknown')}: {str(e)}")
                continue
        
        return migrated_items


# =============================================================================
# L1 INTERFERENCE PATTERN MODEL - PYDANTIC V2
# =============================================================================

class L1InterferencePattern(BaseModel):
    """Modelo para padr√µes de interfer√™ncia L1‚ÜíL2 (portugu√™s‚Üíingl√™s)."""
    pattern_type: str = Field(..., description="Tipo de padr√£o de interfer√™ncia")
    portuguese_structure: str = Field(..., description="Estrutura em portugu√™s")
    incorrect_english: str = Field(..., description="Ingl√™s incorreto (interfer√™ncia)")
    correct_english: str = Field(..., description="Ingl√™s correto")
    explanation: str = Field(..., description="Explica√ß√£o da interfer√™ncia")
    prevention_strategy: str = Field(..., description="Estrat√©gia de preven√ß√£o")
    examples: List[str] = Field(default=[], description="Exemplos adicionais")
    difficulty_level: str = Field(default="intermediate", description="N√≠vel de dificuldade")
    
    # Valida√ß√µes espec√≠ficas
    @field_validator('pattern_type')
    @classmethod
    def validate_pattern_type(cls, v: str) -> str:
        """Validar tipo de padr√£o."""
        valid_types = {
            "grammatical", "lexical", "phonetic", "semantic", 
            "syntactic", "cultural", "pragmatic"
        }
        
        if v.lower() not in valid_types:
            raise ValueError(f"Tipo de padr√£o deve ser um de: {', '.join(valid_types)}")
        
        return v.lower()
    
    @field_validator('difficulty_level')
    @classmethod
    def validate_difficulty_level(cls, v: str) -> str:
        """Validar n√≠vel de dificuldade."""
        valid_levels = {"beginner", "elementary", "intermediate", "upper_intermediate", "advanced"}
        
        if v.lower() not in valid_levels:
            raise ValueError(f"N√≠vel de dificuldade deve ser um de: {', '.join(valid_levels)}")
        
        return v.lower()
    
    @field_validator('prevention_strategy')
    @classmethod
    def validate_prevention_strategy(cls, v: str) -> str:
        """Validar estrat√©gia de preven√ß√£o."""
        valid_strategies = {
            "contrastive_exercises", "awareness_raising", "drilling", 
            "error_correction", "explicit_instruction", "input_enhancement",
            "consciousness_raising", "form_focused_instruction"
        }
        
        if v.lower() not in valid_strategies:
            raise ValueError(f"Estrat√©gia deve ser uma de: {', '.join(valid_strategies)}")
        
        return v.lower()
    
    model_config = {
        "json_schema_extra": {
            "example": {
                "pattern_type": "grammatical",
                "portuguese_structure": "Eu tenho 25 anos",
                "incorrect_english": "I have 25 years",
                "correct_english": "I am 25 years old",
                "explanation": "Portuguese uses 'ter' (have) for age, English uses 'be'",
                "prevention_strategy": "contrastive_exercises",
                "examples": [
                    "I am 30 years old",
                    "She is 25 years old",
                    "How old are you? (not: How many years do you have?)"
                ],
                "difficulty_level": "beginner"
            }
        }
    }


class L1InterferenceAnalysis(BaseModel):
    """An√°lise completa de interfer√™ncia L1‚ÜíL2."""
    grammar_point: str = Field(..., description="Ponto gramatical analisado")
    vocabulary_items: List[str] = Field(..., description="Itens de vocabul√°rio analisados")
    cefr_level: str = Field(..., description="N√≠vel CEFR do conte√∫do")
    
    identified_patterns: List[L1InterferencePattern] = Field(..., description="Padr√µes identificados")
    prevention_strategies: List[str] = Field(..., description="Estrat√©gias de preven√ß√£o gerais")
    common_mistakes: List[str] = Field(..., description="Erros comuns identificados")
    preventive_exercises: List[Dict[str, Any]] = Field(..., description="Exerc√≠cios preventivos sugeridos")
    
    # M√©tricas de an√°lise
    interference_risk_score: float = Field(..., ge=0.0, le=1.0, description="Score de risco de interfer√™ncia")
    patterns_count: int = Field(..., ge=0, description="N√∫mero de padr√µes identificados")
    coverage_areas: List[str] = Field(..., description="√Åreas de interfer√™ncia cobertas")
    
    generated_at: datetime = Field(default_factory=datetime.now)
    
    @field_validator('interference_risk_score')
    @classmethod
    def validate_risk_score(cls, v: float) -> float:
        """Validar score de risco."""
        if not 0.0 <= v <= 1.0:
            raise ValueError("Score de risco deve estar entre 0.0 e 1.0")
        return v
    
    @field_validator('patterns_count')
    @classmethod
    def validate_patterns_count(cls, v: int, info: ValidationInfo) -> int:
        """Validar contagem de padr√µes."""
        if hasattr(info, 'data') and 'identified_patterns' in info.data:
            patterns = info.data['identified_patterns']
            if v != len(patterns):
                return len(patterns)
        return v
    
    model_config = {
        "json_schema_extra": {
            "example": {
                "grammar_point": "Age expressions",
                "vocabulary_items": ["age", "years", "old", "young"],
                "cefr_level": "A1",
                "identified_patterns": [
                    {
                        "pattern_type": "grammatical",
                        "portuguese_structure": "Eu tenho X anos",
                        "incorrect_english": "I have X years",
                        "correct_english": "I am X years old",
                        "explanation": "Age structure difference PT vs EN",
                        "prevention_strategy": "contrastive_exercises"
                    }
                ],
                "prevention_strategies": [
                    "Contrast exercises Portuguese vs English",
                    "Explicit instruction on BE vs HAVE",
                    "Drilling with age expressions"
                ],
                "common_mistakes": [
                    "Using HAVE instead of BE for age",
                    "Literal translation from Portuguese",
                    "Missing 'old' in age expressions"
                ],
                "preventive_exercises": [
                    {
                        "type": "contrast_exercise",
                        "description": "Compare PT and EN age expressions",
                        "examples": ["PT: Tenho 20 anos ‚Üí EN: I am 20 years old"]
                    }
                ],
                "interference_risk_score": 0.8,
                "patterns_count": 1,
                "coverage_areas": ["grammatical_structure", "verb_usage"]
            }
        }
    }


# =============================================================================
# UTILITY FUNCTIONS PARA IPA - NOVAS
# =============================================================================

def extract_phonemes_from_vocabulary(vocabulary_section: VocabularySection) -> List[str]:
    """Extrair lista √∫nica de fonemas de uma se√ß√£o de vocabul√°rio."""
    phonemes = set()
    
    for item in vocabulary_section.items:
        # Extrair fonemas individuais do campo phoneme
        clean_phoneme = item.phoneme.strip('/[]')
        # Separar por espa√ßos e pontos para obter fonemas individuais
        individual_phonemes = clean_phoneme.replace('.', ' ').split()
        
        for phoneme in individual_phonemes:
            if phoneme and len(phoneme) > 0:
                phonemes.add(phoneme)
    
    return sorted(list(phonemes))


def analyze_phonetic_complexity(vocabulary_items: List[VocabularyItem]) -> Dict[str, Any]:
    """Analisar complexidade fon√©tica de uma lista de itens de vocabul√°rio."""
    if not vocabulary_items:
        return {"complexity": "unknown", "details": {}}
    
    syllable_counts = [item.syllable_count for item in vocabulary_items if item.syllable_count]
    phoneme_lengths = [len(item.phoneme.strip('/[]').replace(' ', '')) for item in vocabulary_items]
    
    avg_syllables = sum(syllable_counts) / len(syllable_counts) if syllable_counts else 1
    avg_phoneme_length = sum(phoneme_lengths) / len(phoneme_lengths) if phoneme_lengths else 5
    
    # Determinar complexidade baseada em m√©tricas
    if avg_syllables <= 1.5 and avg_phoneme_length <= 6:
        complexity = "simple"
    elif avg_syllables <= 2.5 and avg_phoneme_length <= 10:
        complexity = "medium"
    elif avg_syllables <= 3.5 and avg_phoneme_length <= 15:
        complexity = "complex"
    else:
        complexity = "very_complex"
    
    return {
        "complexity": complexity,
        "details": {
            "average_syllables": round(avg_syllables, 2),
            "average_phoneme_length": round(avg_phoneme_length, 2),
            "total_items": len(vocabulary_items),
            "syllable_distribution": {
                "1": len([s for s in syllable_counts if s == 1]),
                "2": len([s for s in syllable_counts if s == 2]),
                "3": len([s for s in syllable_counts if s == 3]),
                "4+": len([s for s in syllable_counts if s >= 4])
            }
        }
    }


def validate_ipa_consistency(vocabulary_items: List[VocabularyItem]) -> Dict[str, Any]:
    """Validar consist√™ncia IPA entre itens de vocabul√°rio."""
    variants = set(item.ipa_variant for item in vocabulary_items)
    inconsistencies = []
    
    if len(variants) > 1:
        inconsistencies.append(f"M√∫ltiplas variantes IPA encontradas: {variants}")
    
    # Verificar padr√µes de stress inconsistentes
    stress_patterns = [item.stress_pattern for item in vocabulary_items if item.stress_pattern]
    unique_patterns = set(stress_patterns)
    
    if len(unique_patterns) > 3:
        inconsistencies.append(f"Muitos padr√µes de stress diferentes: {unique_patterns}")
    
    return {
        "is_consistent": len(inconsistencies) == 0,
        "inconsistencies": inconsistencies,
        "variants_used": list(variants),
        "stress_patterns_used": list(unique_patterns)
    }


# =============================================================================
# UTILIDADES PARA L1 INTERFERENCE
# =============================================================================

def create_l1_interference_pattern(
    pattern_type: str,
    portuguese_structure: str,
    incorrect_english: str,
    correct_english: str,
    explanation: str,
    prevention_strategy: str = "contrastive_exercises",
    examples: List[str] = None,
    difficulty_level: str = "intermediate"
) -> L1InterferencePattern:
    """Criar um padr√£o de interfer√™ncia L1‚ÜíL2."""
    return L1InterferencePattern(
        pattern_type=pattern_type,
        portuguese_structure=portuguese_structure,
        incorrect_english=incorrect_english,
        correct_english=correct_english,
        explanation=explanation,
        prevention_strategy=prevention_strategy,
        examples=examples or [],
        difficulty_level=difficulty_level
    )


def get_common_l1_interference_patterns() -> List[L1InterferencePattern]:
    """Retornar padr√µes comuns de interfer√™ncia para brasileiros."""
    return [
        L1InterferencePattern(
            pattern_type="grammatical",
            portuguese_structure="Eu tenho 25 anos",
            incorrect_english="I have 25 years",
            correct_english="I am 25 years old",
            explanation="Portuguese uses 'ter' (have) for age, English uses 'be'",
            prevention_strategy="contrastive_exercises",
            examples=["I am 30 years old", "She is 25 years old"],
            difficulty_level="beginner"
        ),
        L1InterferencePattern(
            pattern_type="lexical",
            portuguese_structure="Eu estou com fome",
            incorrect_english="I am with hunger",
            correct_english="I am hungry",
            explanation="Portuguese uses 'estar com + noun', English uses 'be + adjective'",
            prevention_strategy="explicit_instruction",
            examples=["I am thirsty", "I am tired", "I am cold"],
            difficulty_level="beginner"
        ),
        L1InterferencePattern(
            pattern_type="grammatical",
            portuguese_structure="A Maria √© mais alta que a Ana",
            incorrect_english="Maria is more tall than Ana",
            correct_english="Maria is taller than Ana",
            explanation="Portuguese always uses 'mais + adjective', English has irregular comparatives",
            prevention_strategy="drilling",
        ),
        L1InterferencePattern(
            pattern_type="grammatical",
            portuguese_structure="A Maria √© mais alta que a Ana",
            incorrect_english="Maria is more tall than Ana",
            correct_english="Maria is taller than Ana",
            explanation="Portuguese always uses 'mais + adjective', English has irregular comparatives",
            prevention_strategy="drilling",
            examples=["bigger (not more big)", "better (not more good)"],
            difficulty_level="elementary"
        ),
        L1InterferencePattern(
            pattern_type="phonetic",
            portuguese_structure="Hospital [hos-pi-TAL]",
            incorrect_english="Hospital [hos-pi-TAL]",
            correct_english="Hospital [HOS-pi-tal]",
            explanation="Portuguese stress on final syllable, English on first",
            prevention_strategy="awareness_raising",
            examples=["Hotel [ho-TEL] vs [ho-TEL]", "Animal [a-ni-MAL] vs [AN-i-mal]"],
            difficulty_level="intermediate"
        ),
        L1InterferencePattern(
            pattern_type="semantic",
            portuguese_structure="Pretender fazer algo",
            incorrect_english="I pretend to do something",
            correct_english="I intend to do something",
            explanation="Portuguese 'pretender' = English 'intend', not 'pretend'",
            prevention_strategy="consciousness_raising",
            examples=["I intend to study", "I plan to travel"],
            difficulty_level="intermediate"
        )
        ]


def analyze_text_for_l1_interference(text: str, cefr_level: str) -> List[str]:
    """Analisar texto para poss√≠veis interfer√™ncias L1."""
    common_patterns = get_common_l1_interference_patterns()
    potential_issues = []
    
    text_lower = text.lower()
    
    # Verificar padr√µes conhecidos
    interference_indicators = {
        "have + number + years": "Age expression with HAVE instead of BE",
        "more + adjective": "Comparative with MORE instead of -ER",
        "with + emotion noun": "Emotion expression with WITH instead of adjective",
        "pretend + to": "False friend: pretend vs intend"
    }
    
    for pattern, issue in interference_indicators.items():
        # Verifica√ß√£o simplificada - na pr√°tica seria mais sofisticada
        if any(word in text_lower for word in pattern.split(" + ")):
            potential_issues.append(issue)
    
    return potential_issues


# =============================================================================
# UTILIDADES PARA COMMON MISTAKES
# =============================================================================

def create_common_mistake(
    mistake_type: str,
    incorrect_form: str,
    correct_form: str,
    explanation: str,
    examples: List[str] = None,
    l1_interference: bool = False,
    prevention_strategy: str = "explicit_instruction",
    frequency: str = "medium",
    cefr_level: str = "A2",
    # ‚úÖ NOVOS PAR√ÇMETROS
    context_where_occurs: Optional[str] = None,
    severity_score: Optional[float] = None,
    remedial_exercises: List[str] = None
) -> CommonMistake:
    """Criar um erro comum com par√¢metros expandidos."""
    return CommonMistake(
        mistake_type=mistake_type,
        incorrect_form=incorrect_form,
        correct_form=correct_form,
        explanation=explanation,
        examples=examples or [],
        l1_interference=l1_interference,
        prevention_strategy=prevention_strategy,
        frequency=frequency,
        cefr_level=cefr_level,
        context_where_occurs=context_where_occurs,
        severity_score=severity_score,
        remedial_exercises=remedial_exercises or []
    )


def get_common_brazilian_mistakes() -> List[CommonMistake]:
    """Retornar erros comuns para brasileiros - VERS√ÉO EXPANDIDA."""
    return [
        CommonMistake(
            mistake_type="grammatical",
            incorrect_form="I have 25 years",
            correct_form="I am 25 years old",
            explanation="Portuguese uses 'ter' (have) for age, English uses 'be'",
            examples=[
                "She has 30 years ‚Üí She is 30 years old",
                "My brother has 18 years ‚Üí My brother is 18 years old"
            ],
            l1_interference=True,
            prevention_strategy="contrastive_exercises",
            frequency="very_high",
            cefr_level="A1",
            context_where_occurs="Personal introductions, biographical information",
            age_group_frequency="all_ages",
            severity_score=0.95,
            remedial_exercises=[
                "BE + age drills",
                "How old questions practice",
                "Contrastive Portuguese vs English"
            ]
        ),
        CommonMistake(
            mistake_type="lexical", 
            incorrect_form="I am with hunger",
            correct_form="I am hungry",
            explanation="Portuguese 'estar com fome' vs English adjective",
            examples=[
                "I am with thirst ‚Üí I am thirsty",
                "She is with cold ‚Üí She is cold"
            ],
            l1_interference=True,
            prevention_strategy="pattern_recognition",
            frequency="high",
            cefr_level="A1",
            context_where_occurs="Daily activities, basic needs expression",
            age_group_frequency="all_ages",
            severity_score=0.8,
            remedial_exercises=[
                "BE + adjective practice",
                "Physical state expressions",
                "Contrast exercises: COM vs adjective"
            ]
        ),
        CommonMistake(
            mistake_type="article_omission",
            incorrect_form="The life is beautiful",
            correct_form="Life is beautiful", 
            explanation="Portuguese uses definite article with abstract nouns",
            examples=[
                "The love is important ‚Üí Love is important",
                "The music helps relaxation ‚Üí Music helps relaxation"
            ],
            l1_interference=True,
            prevention_strategy="awareness_raising",
            frequency="high",
            cefr_level="A2",
            context_where_occurs="Abstract concepts, generalizations",
            age_group_frequency="teenagers",
            severity_score=0.7,
            remedial_exercises=[
                "Abstract noun practice",
                "Article omission drills",
                "Generalization statements"
            ]
        ),
        CommonMistake(
            mistake_type="false_friend",
            incorrect_form="I will assist the conference",
            correct_form="I will attend the conference",
            explanation="Portuguese 'assistir' = English 'attend', not 'assist'",
            examples=[
                "I assisted the movie ‚Üí I watched the movie",
                "Did you assist the class? ‚Üí Did you attend the class?"
            ],
            l1_interference=True,
            prevention_strategy="explicit_instruction",
            frequency="medium",
            cefr_level="B1",
            context_where_occurs="Academic and professional contexts",
            age_group_frequency="adults",
            severity_score=0.6,
            remedial_exercises=[
                "False friends identification",
                "Context-based vocabulary practice",
                "Attend vs assist contrast"
            ]
        ),
        CommonMistake(
            mistake_type="pronunciation",
            incorrect_form="/Ààhosp…™tal/ (stress on final)",
            correct_form="/Ààh…ísp…™tl/ (stress on first)",
            explanation="Portuguese stress on final syllable vs English initial stress",
            examples=[
                "hotel: /hoÀàt…õw/ ‚Üí /ho äÀàtel/",
                "animal: /aniÀàmaw/ ‚Üí /Àà√¶n…™m…ôl/"
            ],
            l1_interference=True,
            prevention_strategy="drilling",
            frequency="medium",
            cefr_level="A2",
            context_where_occurs="Cognate words, formal vocabulary",
            age_group_frequency="all_ages",
            severity_score=0.5,
            remedial_exercises=[
                "Stress pattern recognition",
                "Cognate pronunciation drills",
                "Minimal pair practice"
            ]
        )
    ]


# ‚úÖ MELHORIA 7: Fun√ß√£o de an√°lise mais robusta
def analyze_text_for_common_mistakes(
    text: str, 
    cefr_level: str = "A2",
    focus_l1_interference: bool = True
) -> Dict[str, Any]:
    """Analisar texto para identificar erros comuns - VERS√ÉO MELHORADA."""
    common_mistakes = get_common_brazilian_mistakes()
    identified_mistakes = []
    
    text_lower = text.lower()
    
    # Filtrar por n√≠vel CEFR se especificado
    if cefr_level:
        common_mistakes = [
            mistake for mistake in common_mistakes 
            if mistake.cefr_level <= cefr_level
        ]
    
    # Filtrar por interfer√™ncia L1 se especificado
    if focus_l1_interference:
        common_mistakes = [
            mistake for mistake in common_mistakes 
            if mistake.l1_interference
        ]
    
    # Verificar padr√µes de erro conhecidos
    for mistake in common_mistakes:
        # An√°lise mais sofisticada
        incorrect_parts = mistake.incorrect_form.lower().split()
        
        # Verificar se padr√£o existe no texto
        pattern_found = False
        
        # Verifica√ß√£o por palavras-chave
        if len(incorrect_parts) <= 3:
            pattern_found = all(part in text_lower for part in incorrect_parts)
        else:
            # Para padr√µes mais complexos, verificar proximidade
            positions = []
            for part in incorrect_parts:
                if part in text_lower:
                    positions.append(text_lower.find(part))
                else:
                    break
            
            if len(positions) == len(incorrect_parts):
                # Verificar se palavras est√£o pr√≥ximas (dentro de 10 caracteres)
                max_distance = max(positions) - min(positions)
                pattern_found = max_distance <= 20
        
        if pattern_found:
            identified_mistakes.append(mistake)
    
    # An√°lise estat√≠stica
    total_words = len(text.split())
    error_density = len(identified_mistakes) / max(total_words, 1)
    
    # Categorizar erros
    error_categories = {}
    severity_scores = []
    
    for mistake in identified_mistakes:
        category = mistake.mistake_type
        error_categories[category] = error_categories.get(category, 0) + 1
        
        if mistake.severity_score:
            severity_scores.append(mistake.severity_score)
    
    return {
        "identified_mistakes": identified_mistakes,
        "analysis_summary": {
            "total_errors_found": len(identified_mistakes),
            "error_density": round(error_density, 3),
            "average_severity": round(sum(severity_scores) / len(severity_scores), 2) if severity_scores else 0,
            "error_categories": error_categories,
            "l1_interference_errors": len([m for m in identified_mistakes if m.l1_interference]),
            "most_common_error_type": max(error_categories.keys(), key=error_categories.get) if error_categories else None
        },
        "recommendations": [
            f"Focus on {mistake.prevention_strategy} for {mistake.mistake_type} errors"
            for mistake in identified_mistakes[:3]
        ],
        "text_analysis": {
            "word_count": total_words,
            "cefr_level_analyzed": cefr_level,
            "l1_interference_focus": focus_l1_interference
        }
    }


# =============================================================================
# FUN√á√ïES UTILIT√ÅRIAS ADICIONAIS
# =============================================================================

def get_mistakes_by_cefr_level(cefr_level: str) -> List[CommonMistake]:
    """Obter erros comuns para um n√≠vel CEFR espec√≠fico."""
    all_mistakes = get_common_brazilian_mistakes()
    return [mistake for mistake in all_mistakes if mistake.cefr_level == cefr_level.upper()]


def get_mistakes_by_type(mistake_type: str) -> List[CommonMistake]:
    """Obter erros comuns por tipo."""
    all_mistakes = get_common_brazilian_mistakes()
    return [mistake for mistake in all_mistakes if mistake.mistake_type == mistake_type.lower()]


def get_high_priority_mistakes() -> List[CommonMistake]:
    """Obter erros de alta prioridade (frequency=high/very_high, severity>0.7)."""
    all_mistakes = get_common_brazilian_mistakes()
    return [
        mistake for mistake in all_mistakes 
        if mistake.frequency in ["high", "very_high"] and 
           (mistake.severity_score or 0) > 0.7
    ]


# =============================================================================
# VALIDA√á√ÉO E STATUS
# =============================================================================

def validate_common_mistake_structure(mistake_data: dict) -> Dict[str, Any]:
    """Validar estrutura de dados de erro comum."""
    try:
        mistake = CommonMistake(**mistake_data)
        return {
            "valid": True,
            "validated_mistake": mistake,
            "validation_errors": []
        }
    except ValidationError as e:
        return {
            "valid": False,
            "validated_mistake": None,
            "validation_errors": [str(error) for error in e.errors()]
        }


# Constrastive example
class ContrastiveExample(BaseModel):
    """
    Exemplo contrastivo para an√°lise estrutural portugu√™s‚Üîingl√™s.
    Diferente de CommonMistake - foca em PREVEN√á√ÉO via contraste estrutural.
    
    Usado pelo L1InterferenceAnalyzer para an√°lise preventiva.
    """
    
    # Estruturas contrastivas
    portuguese: str = Field(..., description="Vers√£o/estrutura em portugu√™s")
    english_wrong: str = Field(..., description="Ingl√™s incorreto (transfer√™ncia literal)")
    english_correct: str = Field(..., description="Ingl√™s correto")
    
    # An√°lise pedag√≥gica
    teaching_point: str = Field(..., description="Ponto de ensino principal")
    structural_difference: str = Field(..., description="Diferen√ßa estrutural espec√≠fica")
    interference_type: str = Field(..., description="Tipo de interfer√™ncia")
    
    # Contexto pedag√≥gico
    cefr_level: str = Field(default="A2", description="N√≠vel CEFR relevante")
    difficulty_level: str = Field(default="medium", description="N√≠vel de dificuldade")
    prevention_strategy: str = Field(default="contrastive_awareness", description="Estrat√©gia de preven√ß√£o")
    
    # Exemplos pr√°ticos
    additional_examples: List[str] = Field(default=[], description="Exemplos adicionais")
    practice_sentences: List[str] = Field(default=[], description="Frases para pr√°tica")
    
    # Metadados
    linguistic_explanation: Optional[str] = Field(None, description="Explica√ß√£o lingu√≠stica detalhada")
    common_in_context: Optional[str] = Field(None, description="Contexto onde √© comum")
    
    @field_validator('interference_type')
    @classmethod
    def validate_interference_type(cls, v: str) -> str:
        """Validar tipo de interfer√™ncia."""
        valid_types = {
            "grammatical_structure",     # Diferen√ßas gramaticais
            "word_order",               # Ordem das palavras
            "article_usage",            # Uso de artigos
            "verb_construction",        # Constru√ß√£o verbal
            "preposition_pattern",      # Padr√µes de preposi√ß√£o
            "pronoun_usage",           # Uso de pronomes
            "tense_aspect",            # Tempo e aspecto
            "modality_expression",     # Express√£o de modalidade
            "negation_pattern",        # Padr√µes de nega√ß√£o
            "question_formation",      # Forma√ß√£o de perguntas
            "comparative_structure",   # Estruturas comparativas
            "possession_expression"    # Express√£o de posse
        }
        
        if v not in valid_types:
            raise ValueError(f"Tipo de interfer√™ncia deve ser um de: {', '.join(valid_types)}")
        
        return v
    
    @field_validator('difficulty_level')
    @classmethod
    def validate_difficulty_level(cls, v: str) -> str:
        """Validar n√≠vel de dificuldade."""
        valid_levels = {"very_easy", "easy", "medium", "hard", "very_hard"}
        
        if v not in valid_levels:
            raise ValueError(f"N√≠vel de dificuldade deve ser um de: {', '.join(valid_levels)}")
        
        return v
    
    @field_validator('prevention_strategy')
    @classmethod
    def validate_prevention_strategy(cls, v: str) -> str:
        """Validar estrat√©gia de preven√ß√£o."""
        valid_strategies = {
            "contrastive_awareness",     # Conscientiza√ß√£o contrastiva
            "explicit_instruction",     # Instru√ß√£o expl√≠cita
            "pattern_recognition",      # Reconhecimento de padr√µes
            "controlled_practice",      # Pr√°tica controlada
            "error_anticipation",       # Antecipa√ß√£o de erros
            "structural_comparison",    # Compara√ß√£o estrutural
            "metalinguistic_awareness", # Consci√™ncia metalingu√≠stica
            "form_focused_instruction"  # Instru√ß√£o focada na forma
        }
        
        if v not in valid_strategies:
            raise ValueError(f"Estrat√©gia deve ser uma de: {', '.join(valid_strategies)}")
        
        return v
    
    model_config = {
        "json_schema_extra": {
            "example": {
                "portuguese": "Eu tenho 25 anos",
                "english_wrong": "I have 25 years",
                "english_correct": "I am 25 years old",
                "teaching_point": "Age expression uses BE + years old, not HAVE + years",
                "structural_difference": "Portuguese uses HAVE + age, English uses BE + age + 'years old'",
                "interference_type": "verb_construction",
                "cefr_level": "A1",
                "difficulty_level": "medium",
                "prevention_strategy": "contrastive_awareness",
                "additional_examples": [
                    "She is 30 years old (not: She has 30 years)",
                    "How old are you? (not: How many years do you have?)"
                ],
                "practice_sentences": [
                    "My brother ___ 22 years old. (is)",
                    "How old ___ your sister? (is)"
                ],
                "linguistic_explanation": "Portuguese 'ter idade' vs English 'be age years old' represents different conceptualization of age as possession vs state",
                "common_in_context": "Basic personal information, introductions"
            }
        }
    }


class ContrastiveExampleSection(BaseModel):
    """Se√ß√£o de exemplos contrastivos para uma unidade - AN√ÅLISE ESTRUTURAL."""
    examples: List[ContrastiveExample] = Field(..., description="Lista de exemplos contrastivos")
    total_examples: int = Field(..., description="Total de exemplos")
    
    # An√°lise da se√ß√£o
    main_interference_types: List[str] = Field(default=[], description="Principais tipos de interfer√™ncia")
    prevention_focus: str = Field(..., description="Foco principal de preven√ß√£o")
    difficulty_assessment: str = Field(default="medium", description="Avalia√ß√£o de dificuldade geral")
    
    # Recomenda√ß√µes pedag√≥gicas
    teaching_sequence: List[str] = Field(default=[], description="Sequ√™ncia recomendada de ensino")
    practice_activities: List[str] = Field(default=[], description="Atividades de pr√°tica sugeridas")
    
    # Metadados
    target_cefr_level: str = Field(..., description="N√≠vel CEFR alvo")
    brazilian_learner_focus: bool = Field(True, description="Foco em aprendizes brasileiros")
    
    generated_at: datetime = Field(default_factory=datetime.now)
    
    @field_validator('total_examples')
    @classmethod
    def validate_total_examples(cls, v: int, info: ValidationInfo) -> int:
        """Validar contagem total."""
        if hasattr(info, 'data') and 'examples' in info.data:
            examples = info.data['examples']
            if v != len(examples):
                return len(examples)
        return v
    
    model_config = {
        "json_schema_extra": {
            "example": {
                "examples": [
                    {
                        "portuguese": "Eu tenho 25 anos",
                        "english_wrong": "I have 25 years", 
                        "english_correct": "I am 25 years old",
                        "teaching_point": "Age expression difference",
                        "structural_difference": "Portuguese TER vs English BE",
                        "interference_type": "verb_construction"
                    }
                ],
                "total_examples": 1,
                "main_interference_types": ["verb_construction"],
                "prevention_focus": "Structural awareness of PT vs EN verb usage",
                "difficulty_assessment": "medium",
                "teaching_sequence": [
                    "Present Portuguese structure",
                    "Show English equivalent", 
                    "Highlight difference",
                    "Practice correct form"
                ],
                "practice_activities": [
                    "Contrastive comparison exercises",
                    "Error identification tasks",
                    "Controlled production practice"
                ],
                "target_cefr_level": "A1",
                "brazilian_learner_focus": True
            }
        }
    }


# =============================================================================
# UTILITY FUNCTIONS PARA CONTRASTIVE EXAMPLES
# =============================================================================

def create_contrastive_example(
    portuguese: str,
    english_wrong: str,
    english_correct: str,
    teaching_point: str,
    structural_difference: str = "",
    interference_type: str = "grammatical_structure",
    cefr_level: str = "A2",
    difficulty_level: str = "medium",
    prevention_strategy: str = "contrastive_awareness",
    additional_examples: List[str] = None,
    practice_sentences: List[str] = None
) -> ContrastiveExample:
    """Criar um exemplo contrastivo estruturado."""
    return ContrastiveExample(
        portuguese=portuguese,
        english_wrong=english_wrong,
        english_correct=english_correct,
        teaching_point=teaching_point,
        structural_difference=structural_difference or f"Portuguese vs English structural difference",
        interference_type=interference_type,
        cefr_level=cefr_level,
        difficulty_level=difficulty_level,
        prevention_strategy=prevention_strategy,
        additional_examples=additional_examples or [],
        practice_sentences=practice_sentences or []
    )


def get_common_contrastive_examples_for_brazilians() -> List[ContrastiveExample]:
    """Retornar exemplos contrastivos comuns para brasileiros."""
    return [
        ContrastiveExample(
            portuguese="Eu tenho 25 anos",
            english_wrong="I have 25 years",
            english_correct="I am 25 years old",
            teaching_point="Age expression uses BE + years old, not HAVE + years",
            structural_difference="Portuguese: TER + idade | English: BE + idade + years old",
            interference_type="verb_construction",
            cefr_level="A1",
            difficulty_level="medium",
            prevention_strategy="contrastive_awareness",
            additional_examples=[
                "She is 30 years old (not: She has 30 years)",
                "How old are you? (not: How many years do you have?)"
            ],
            practice_sentences=[
                "My brother ___ 22 years old. (is)",
                "How old ___ your sister? (is)"
            ]
        ),
        ContrastiveExample(
            portuguese="Eu estou com fome",
            english_wrong="I am with hunger",
            english_correct="I am hungry",
            teaching_point="Emotions/states use adjectives, not 'with + noun'",
            structural_difference="Portuguese: ESTAR COM + substantivo | English: BE + adjetivo",
            interference_type="grammatical_structure",
            cefr_level="A1",
            difficulty_level="easy",
            prevention_strategy="pattern_recognition",
            additional_examples=[
                "I am thirsty (not: I am with thirst)",
                "I am tired (not: I am with tiredness)"
            ],
            practice_sentences=[
                "She is ___ after the long walk. (tired)",
                "Are you ___? There's food in the kitchen. (hungry)"
            ]
        ),
        ContrastiveExample(
            portuguese="A vida √© bela",
            english_wrong="The life is beautiful",
            english_correct="Life is beautiful",
            teaching_point="Abstract nouns don't need definite article in generalizations",
            structural_difference="Portuguese: artigo + substantivo abstrato | English: substantivo abstrato (sem artigo)",
            interference_type="article_usage",
            cefr_level="A2",
            difficulty_level="hard",
            prevention_strategy="explicit_instruction",
            additional_examples=[
                "Love is important (not: The love is important)",
                "Music is universal (not: The music is universal)"
            ],
            practice_sentences=[
                "___ is the key to happiness. (Love)",
                "___ helps us relax. (Music)"
            ]
        ),
        ContrastiveExample(
            portuguese="Ela √© mais alta que eu",
            english_wrong="She is more tall than me",
            english_correct="She is taller than me",
            teaching_point="Short adjectives use -er, not 'more + adjective'",
            structural_difference="Portuguese: MAIS + adjetivo | English: adjetivo-ER (para adj. curtos)",
            interference_type="comparative_structure",
            cefr_level="A2",
            difficulty_level="medium",
            prevention_strategy="pattern_recognition",
            additional_examples=[
                "He is faster than his brother (not: more fast)",
                "This book is better than that one (not: more good)"
            ],
            practice_sentences=[
                "This car is ___ than mine. (faster)",
                "My house is ___ than yours. (bigger)"
            ]
        )
    ]


def analyze_contrastive_pattern(
    portuguese_structure: str,
    english_structure: str,
    examples: List[str] = None
) -> Dict[str, Any]:
    """Analisar padr√£o contrastivo entre portugu√™s e ingl√™s."""
    return {
        "portuguese_pattern": portuguese_structure,
        "english_pattern": english_structure,
        "structural_differences": [
            "Analyze word order differences",
            "Analyze verb construction differences", 
            "Analyze article usage differences",
            "Analyze preposition usage differences"
        ],
        "interference_likelihood": "high" if "ter" in portuguese_structure.lower() else "medium",
        "teaching_recommendations": [
            "Use explicit contrastive explanation",
            "Provide controlled practice",
            "Create awareness activities",
            "Use error anticipation exercises"
        ],
        "examples_provided": examples or [],
        "analysis_confidence": 0.85
    }
# =============================================================================
# FORWARD REFERENCES FIX - PYDANTIC V2 COMPATIBLE
# =============================================================================

# Resolver refer√™ncias circulares para Pydantic V2
UnitResponse.model_rebuild()
VocabularySection.model_rebuild()
SentencesSection.model_rebuild()
QASection.model_rebuild()


# =============================================================================
# EXPORTS E VERSIONING
# =============================================================================

__all__ = [
    # Input Models
    "UnitCreateRequest",
    
    # Vocabulary Models
    "VocabularyItem",
    "VocabularySection",
    "VocabularyGenerationRequest", 
    "VocabularyGenerationResponse",
    
    # Content Generation Request Models
    "SentenceGenerationRequest",
    "TipsGenerationRequest", 
    "AssessmentGenerationRequest",
    "QAGenerationRequest",
    "GrammarGenerationRequest",
    
    # Content Models
    "TipsContent",
    "GrammarContent",
    
    # Assessment Models
    "AssessmentActivity",
    "AssessmentSection",
    
    # Common Mistake Models - NOVO
    "CommonMistake",
    "CommonMistakeSection",
    
    # Unit Models
    "UnitResponse",
    
    # Additional Content Models
    "Sentence",
    "SentencesSection",
    "QASection",
    "ImageInfo",
    
    # Progress Models
    "GenerationProgress",
    "ErrorResponse",
    "SuccessResponse",
    
    # Bulk Models
    "BulkUnitStatus",
    "CourseStatistics",
    
    # Phonetic Models
    "PhoneticValidationResult",
    "BulkPhoneticValidation",
    
    # L1 Interference Models
    "L1InterferencePattern",
    "L1InterferenceAnalysis",
    
    # Legacy Models
    "LegacyUnitAdapter",
    
    # Utility Functions
    "extract_phonemes_from_vocabulary",
    "analyze_phonetic_complexity",
    "validate_ipa_consistency",
    "create_l1_interference_pattern",
    "get_common_l1_interference_patterns",
    "analyze_text_for_l1_interference",
    "create_common_mistake",
    "get_common_brazilian_mistakes",
    "analyze_text_for_common_mistakes",

    #Constrative Example Models
    "ContrastiveExample",
    "ContrastiveExampleSection", 
    "create_contrastive_example",
    "get_common_contrastive_examples_for_brazilians",
    "analyze_contrastive_pattern"
]

# Versioning para Pydantic V2
__pydantic_version__ = "2.x"
__compatibility__ = "Pydantic V2 Compatible"
__migration_date__ = "2025-01-28"
__breaking_changes__ = [
    "@validator ‚Üí @field_validator",
    "class Config ‚Üí model_config", 
    "schema_extra ‚Üí json_schema_extra",
    "values ‚Üí info.data in validators"
]