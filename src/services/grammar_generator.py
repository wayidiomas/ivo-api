"""
Gerador de conte√∫do gramatical usando IA - LangChain 0.3.
Implementa estrat√©gias GRAMMAR 1 (Sistem√°tica) e GRAMMAR 2 (Preven√ß√£o L1‚ÜíL2).
CORRIGIDO: Prompts contextuais via IA, integra√ß√£o com sistema IVO V2.
"""
import logging
from pathlib import Path
from typing import Dict, List, Optional, Any
import json
import asyncio
from dataclasses import dataclass

# YAML import
import yaml

from langchain_openai import ChatOpenAI
from langchain_core.messages import SystemMessage, HumanMessage

# Pydantic 2 nativo - sem necessidade de compatibilidade
from pydantic import BaseModel, ValidationError, Field, ConfigDict

# Imports internos
from config.logger_config import get_logger
from config.models import get_openai_config, load_model_configs
from src.core.unit_models import GrammarContent
from src.core.enums import GrammarStrategy

# Logger configurado
logger = get_logger("grammar_generator")
logger.info("üöÄ Usando LangChain 0.3 com Pydantic 2 nativo, IA contextual e structured output")


# =============================================================================
# CONSTANTES T√âCNICAS (MANTIDAS - PADR√ïES ESTABELECIDOS)
# =============================================================================

GRAMMAR_STRATEGIES = {
    "systematic": "explicacao_sistematica",
    "l1_prevention": "prevencao_erros_l1"
}

CEFR_LEVELS = ["A1", "A2", "B1", "B2", "C1", "C2"]

LANGUAGE_VARIANTS = ["american", "british", "australian", "canadian"]


# Removi @dataclass GrammarContent - agora usando o modelo Pydantic de src.core.unit_models


class GrammarRequest(BaseModel):
    """Modelo de requisi√ß√£o para gera√ß√£o de gram√°tica - Pydantic 2."""
    input_text: str = Field(..., description="Texto base para an√°lise gramatical")
    vocabulary_list: List[str] = Field(..., description="Lista de vocabul√°rio dispon√≠vel") 
    level: str = Field(..., description="N√≠vel CEFR (A1, A2, B1, B2, C1, C2)")
    variant: str = Field(default="american", description="Variante do ingl√™s")
    unit_context: str = Field(default="", description="Contexto espec√≠fico da unidade")
    strategy: str = Field(default="systematic", description="Estrat√©gia: systematic ou l1_prevention")
    rag_context: Dict[str, Any] = Field(default_factory=dict, description="Contexto RAG da hierarquia")

    # üî• Pydantic 2 - Nova sintaxe de configura√ß√£o
    model_config = ConfigDict(
        str_strip_whitespace=True,
        validate_assignment=True,
        arbitrary_types_allowed=True
    )


class GrammarGenerator:
    """Gerador de conte√∫do gramatical contextual - LangChain 0.3 + IA."""
    
    def __init__(self):
        """Inicializar gerador com LangChain 0.3 e IA contextual."""
        self.llm = None
        self._load_config()
        
    def _load_config(self):
        """Carregar configura√ß√µes para LangChain 0.3 com modelo correto."""
        try:
            # NOVO: Usar model_selector para obter o modelo correto
            from src.services.model_selector import get_llm_config_for_service
            
            # Obter configura√ß√£o espec√≠fica para grammar_generator
            llm_config = get_llm_config_for_service("grammar_generator")
            
            # üîß Par√¢metros para LangChain 0.3 com modelo correto (o3-mini)
            self.llm = ChatOpenAI(**llm_config)
            
            logger.info(f"‚úÖ ChatOpenAI v0.3 configurado com modelo: {llm_config.get('model')} (TIER-3: Racioc√≠nio Pedag√≥gico)")
                
        except Exception as e:
            logger.error(f"‚ùå Erro na configura√ß√£o: {e}")
            raise

    async def generate_grammar_content(self, request: GrammarRequest) -> GrammarContent:
        """
        Gerar conte√∫do gramatical contextual - LangChain 0.3 + IA.
        
        Args:
            request: Dados da requisi√ß√£o validados pelo Pydantic 2
            
        Returns:
            GrammarContent: Conte√∫do estruturado por estrat√©gia
        """
        try:
            logger.info(f"üéØ Gerando gram√°tica {request.level} - Estrat√©gia: {request.strategy}")
            
            # Valida√ß√£o autom√°tica pelo Pydantic 2
            if not request.input_text.strip():
                raise ValueError("Texto de entrada vazio")
            
            # AN√ÅLISE VIA IA: Identificar ponto gramatical principal
            grammar_point = await self._identify_grammar_point_ai(
                text=request.input_text,
                vocabulary=request.vocabulary_list,
                context=request.unit_context,
                level=request.level
            )
            
            # AN√ÅLISE VIA IA: Gerar prompt contextual baseado na estrat√©gia
            contextual_messages = await self._generate_contextual_prompt_ai(
                request=request,
                grammar_point=grammar_point
            )
            
            # üöÄ LangChain 0.3 - M√©todo ainvoke moderno
            logger.debug("üîÑ Invocando LLM com prompt contextual (LangChain 0.3)")
            response = await self.llm.ainvoke(contextual_messages)
            
            # Extrair conte√∫do da resposta
            content = response.content if hasattr(response, 'content') else str(response)
            
            # üîÑ Usar structured output para garantir formato correto
            grammar_content = await self._generate_grammar_with_structured_output(
                contextual_messages=contextual_messages,
                request=request,
                grammar_point=grammar_point
            )
            
            logger.info(f"‚úÖ Gram√°tica gerada: {grammar_point} ({request.strategy})")
            return grammar_content
            
        except Exception as e:
            logger.error(f"‚ùå Erro na gera√ß√£o de gram√°tica: {str(e)}")
            raise
    
    def _create_grammar_schema(self) -> Dict[str, Any]:
        """Create precise JSON schema for GrammarContent using LangChain 0.3 structured output."""
        return {
            "title": "GrammarContent",
            "description": "Schema for structured grammar content generation with L1 interference analysis and systematic explanations",
            "type": "object",
            "properties": {
                "grammar_point": {
                    "type": "string",
                    "description": "Main grammar point"
                },
                "systematic_explanation": {
                    "type": "string",
                    "description": "Clear and detailed systematic explanation"
                },
                "usage_rules": {
                    "type": "array",
                    "items": {"type": "string"},
                    "minItems": 2,
                    "description": "List of usage rules as simple strings"
                },
                "examples": {
                    "type": "array",
                    "items": {"type": "string"},
                    "minItems": 3,
                    "description": "List of contextualized examples as simple strings"
                },
                "l1_interference_notes": {
                    "type": "array",
                    "items": {"type": "string"},
                    "description": "List of L1 interference notes as simple strings"
                },
                "common_mistakes": {
                    "type": "array",
                    "items": {
                        "type": "object",
                        "properties": {
                            "mistake": {"type": "string", "description": "Common mistake"},
                            "correction": {"type": "string", "description": "Correction"},
                            "explanation": {"type": "string", "description": "Error explanation"}
                        },
                        "required": ["mistake", "correction", "explanation"],
                        "additionalProperties": False
                    },
                    "description": "List of common mistakes and their corrections"
                },
                "vocabulary_integration": {
                    "type": "array",
                    "items": {"type": "string"},
                    "default": [],
                    "description": "How it integrates with vocabulary"
                },
                "previous_grammar_connections": {
                    "type": "array",
                    "items": {"type": "string"},
                    "default": [],
                    "description": "Connections to previous grammar"
                },
                "selection_rationale": {
                    "type": "string",
                    "default": "",
                    "description": "Why this strategy was selected"
                }
            },
            "required": ["grammar_point", "systematic_explanation", "usage_rules", "examples", "l1_interference_notes", "common_mistakes"],
            "additionalProperties": False
        }
    
    async def _generate_grammar_with_structured_output(
        self, 
        contextual_messages: List[Any],
        request: GrammarRequest,
        grammar_point: str
    ) -> GrammarContent:
        """Gerar conte√∫do gramatical usando LangChain 0.3 structured output."""
        try:
            logger.info("ü§ñ Gerando conte√∫do gramatical com structured output...")
            
            # Usar LangChain 0.3 with_structured_output para for√ßar formato correto
            grammar_schema = self._create_grammar_schema()
            structured_llm = self.llm.with_structured_output(grammar_schema)
            
            # Gerar usando structured output
            grammar_data = await structured_llm.ainvoke(contextual_messages)
            
            # Validar que retornou dict
            if not isinstance(grammar_data, dict):
                logger.warning("‚ö†Ô∏è Structured output n√£o retornou dict, convertendo...")
                grammar_data = dict(grammar_data) if hasattr(grammar_data, '__dict__') else {}
            
            # Garantir campos obrigat√≥rios com fallbacks seguros
            grammar_data = self._ensure_grammar_required_fields(grammar_data, grammar_point, request)
            
            # Validar estrutura dos dados
            grammar_data = self._clean_grammar_data(grammar_data)
            
            # Determinar estrat√©gia baseada no request
            strategy = GrammarStrategy.EXPLICACAO_SISTEMATICA
            if request.strategy == "l1_prevention":
                strategy = GrammarStrategy.PREVENCAO_ERROS_L1
            
            # Criar objeto GrammarContent
            grammar_content = GrammarContent(
                strategy=strategy,
                grammar_point=grammar_data["grammar_point"],
                systematic_explanation=grammar_data["systematic_explanation"],
                usage_rules=grammar_data["usage_rules"],
                examples=grammar_data["examples"],
                l1_interference_notes=grammar_data["l1_interference_notes"],
                common_mistakes=grammar_data["common_mistakes"],
                vocabulary_integration=grammar_data.get("vocabulary_integration", []),
                previous_grammar_connections=grammar_data.get("previous_grammar_connections", []),
                selection_rationale=grammar_data.get("selection_rationale", "")
            )
            
            logger.info(
                f"‚úÖ LLM retornou conte√∫do gramatical estruturado: "
                f"{len(grammar_data.get('usage_rules', []))} regras, "
                f"{len(grammar_data.get('examples', []))} exemplos, "
                f"{len(grammar_data.get('common_mistakes', []))} erros comuns"
            )
            return grammar_content
            
        except Exception as e:
            logger.error(f"‚ùå Erro na gera√ß√£o com structured output: {str(e)}")
            logger.info("üîÑ Tentando fallback sem structured output...")
            return await self._generate_grammar_fallback(contextual_messages, request, grammar_point)
    
    def _ensure_grammar_required_fields(
        self, 
        grammar_data: Dict[str, Any], 
        grammar_point: str, 
        request: GrammarRequest
    ) -> Dict[str, Any]:
        """Garantir campos obrigat√≥rios no conte√∫do gramatical."""
        # Garantir campo principal
        if "grammar_point" not in grammar_data:
            grammar_data["grammar_point"] = grammar_point
        
        # Garantir explica√ß√£o
        if "systematic_explanation" not in grammar_data:
            grammar_data["systematic_explanation"] = f"Explica√ß√£o sistem√°tica de {grammar_point}"
        
        # Garantir listas obrigat√≥rias
        required_lists = {
            "usage_rules": [f"Regra principal de {grammar_point}", "Aplica√ß√£o em contexto espec√≠fico"],
            "examples": [f"Example using {grammar_point}", "Practical application example", "Contextual usage example"],
            "l1_interference_notes": [f"Poss√≠vel interfer√™ncia L1 em {grammar_point}"],
            "common_mistakes": [
                {"mistake": f"Common error with {grammar_point}", "correction": "Correct form", "explanation": "Why this is incorrect"}
            ]
        }
        
        for field, default_value in required_lists.items():
            if field not in grammar_data or not isinstance(grammar_data[field], list):
                grammar_data[field] = default_value
        
        # Garantir campos opcionais
        optional_fields = {
            "vocabulary_integration": [],
            "previous_grammar_connections": [],
            "selection_rationale": f"Estrat√©gia {request.strategy} selecionada para {grammar_point}"
        }
        
        for field, default_value in optional_fields.items():
            if field not in grammar_data:
                grammar_data[field] = default_value
        
        return grammar_data
    
    def _clean_grammar_data(self, grammar_data: Dict[str, Any]) -> Dict[str, Any]:
        """Limpar e validar estrutura dos dados gramaticais."""
        # Limpar arrays de strings
        string_arrays = ["usage_rules", "examples", "l1_interference_notes", "vocabulary_integration", "previous_grammar_connections"]
        
        for field in string_arrays:
            if field in grammar_data:
                grammar_data[field] = self._ensure_string_list(grammar_data[field])
        
        # Validar e limpar common_mistakes
        if "common_mistakes" in grammar_data and isinstance(grammar_data["common_mistakes"], list):
            cleaned_mistakes = []
            for mistake in grammar_data["common_mistakes"]:
                if isinstance(mistake, dict):
                    cleaned_mistake = {
                        "mistake": str(mistake.get("mistake", "Common error")).strip(),
                        "correction": str(mistake.get("correction", "Correct form")).strip(),
                        "explanation": str(mistake.get("explanation", "Explanation of error")).strip()
                    }
                    cleaned_mistakes.append(cleaned_mistake)
            grammar_data["common_mistakes"] = cleaned_mistakes
        
        return grammar_data
    
    def _ensure_string_list(self, value: Any) -> List[str]:
        """Garantir que valor seja lista de strings v√°lidas."""
        if not isinstance(value, list):
            return []
        
        result = []
        for item in value:
            if item and str(item).strip():
                result.append(str(item).strip())
        
        return result
    
    async def _generate_grammar_fallback(
        self, 
        contextual_messages: List[Any],
        request: GrammarRequest,
        grammar_point: str
    ) -> GrammarContent:
        """Fallback para gera√ß√£o sem structured output quando structured falha."""
        try:
            logger.info("üîÑ Tentando gera√ß√£o fallback sem structured output...")
            
            # Gerar usando LangChain tradicional
            response = await self.llm.ainvoke(contextual_messages)
            content = response.content if hasattr(response, 'content') else str(response)
            
            # Usar parser IA original como fallback
            grammar_content = await self._parse_grammar_response_ai(
                content=content,
                request=request,
                grammar_point=grammar_point
            )
            
            logger.info(f"‚úÖ Fallback gerou conte√∫do gramatical para {grammar_point}")
            return grammar_content
                
        except Exception as e:
            logger.error(f"‚ùå Erro no fallback: {str(e)}")
            # Criar conte√∫do m√≠nimo de emerg√™ncia
            return self._create_minimal_grammar_content(grammar_point, request)
    
    def _create_minimal_grammar_content(self, grammar_point: str, request: GrammarRequest) -> GrammarContent:
        """Criar conte√∫do gramatical m√≠nimo de emerg√™ncia."""
        strategy = GrammarStrategy.EXPLICACAO_SISTEMATICA
        if request.strategy == "l1_prevention":
            strategy = GrammarStrategy.PREVENCAO_ERROS_L1
        
        return GrammarContent(
            strategy=strategy,
            grammar_point=grammar_point,
            systematic_explanation=f"Explica√ß√£o b√°sica de {grammar_point}",
            usage_rules=[f"Regra principal de {grammar_point}", "Aplica√ß√£o em contexto"],
            examples=[f"Example with {grammar_point}", "Practical usage example"],
            l1_interference_notes=[f"Poss√≠vel interfer√™ncia L1 com {grammar_point}"],
            common_mistakes=[{
                "mistake": f"Common error with {grammar_point}",
                "correction": "Correct form",
                "explanation": "Explanation of the error"
            }],
            vocabulary_integration=[],
            previous_grammar_connections=[],
            selection_rationale=f"Estrat√©gia {request.strategy} aplicada"
        )

    # =============================================================================
    # AN√ÅLISES VIA IA (SUBSTITUEM PROMPTS HARD-CODED)
    # =============================================================================

    async def _identify_grammar_point_ai(
        self, 
        text: str, 
        vocabulary: List[str], 
        context: str, 
        level: str
    ) -> str:
        """Identificar ponto gramatical principal via an√°lise IA."""
        
        system_prompt = """Voc√™ √© um especialista em an√°lise gramatical contextual.
        
        Identifique o ponto gramatical mais relevante e produtivo considerando o texto, vocabul√°rio e contexto espec√≠ficos."""
        
        human_prompt = f"""Identifique o ponto gramatical principal:
        
        TEXTO: {text}
        VOCABUL√ÅRIO: {', '.join(vocabulary[:10])}
        CONTEXTO DA UNIDADE: {context}
        N√çVEL CEFR: {level}
        
        Analise e determine qual ponto gramatical seria mais relevante e pedagogicamente produtivo para esta situa√ß√£o espec√≠fica.
        
        Considere:
        - Estruturas presentes no texto
        - Vocabul√°rio dispon√≠vel para exemplos
        - Adequa√ß√£o ao n√≠vel {level}
        - Relev√¢ncia para o contexto "{context}"
        
        Retorne APENAS o nome do ponto gramatical (ex: "Present Perfect", "Modal Verbs", "Conditional Sentences")."""
        
        try:
            messages = [
                SystemMessage(content=system_prompt),
                HumanMessage(content=human_prompt)
            ]
            
            response = await self.llm.ainvoke(messages)
            grammar_point = response.content.strip()
            
            # Valida√ß√£o b√°sica
            if len(grammar_point) > 100:
                grammar_point = grammar_point[:100]
            
            return grammar_point if grammar_point else "Grammar Structures"
            
        except Exception as e:
            logger.warning(f"Erro na identifica√ß√£o gramatical via IA: {str(e)}")
            return "Grammar Structures"

    async def _generate_contextual_prompt_ai(
        self, 
        request: GrammarRequest, 
        grammar_point: str
    ) -> List[Any]:
        """Gerar prompt contextual espec√≠fico via IA baseado na estrat√©gia."""
        
        # Determinar tipo de estrat√©gia
        if request.strategy == "l1_prevention":
            return await self._generate_l1_prevention_prompt(request, grammar_point)
        else:
            return await self._generate_systematic_prompt(request, grammar_point)

    async def _generate_systematic_prompt(
        self, 
        request: GrammarRequest, 
        grammar_point: str
    ) -> List[Any]:
        """Gerar prompt para GRAMMAR 1: Explica√ß√£o Sistem√°tica."""
        
        # AN√ÅLISE VIA IA: Abordagem sistem√°tica espec√≠fica
        systematic_approach = await self._analyze_systematic_approach_ai(
            grammar_point=grammar_point,
            level=request.level,
            context=request.unit_context,
            vocabulary=request.vocabulary_list
        )
        
        system_prompt = f"""Voc√™ √© um especialista em ensino sistem√°tico de gram√°tica inglesa.
        
        Sua tarefa √© criar explica√ß√£o sistem√°tica e estruturada do ponto gramatical, adaptada ao contexto espec√≠fico.
        
        ESTRAT√âGIA: GRAMMAR 1 - Explica√ß√£o Sistem√°tica
        ABORDAGEM CONTEXTUAL: {systematic_approach}"""
        
        human_prompt = f"""Crie explica√ß√£o sistem√°tica para:
        
        PONTO GRAMATICAL: {grammar_point}
        TEXTO BASE: {request.input_text}
        VOCABUL√ÅRIO: {', '.join(request.vocabulary_list[:10])}
        N√çVEL: {request.level}
        CONTEXTO: {request.unit_context}
        VARIANTE: {request.variant}
        
        FORMATO ESTRUTURADO:
        1. EXPLICA√á√ÉO CLARA: Regra gramatical adaptada ao n√≠vel {request.level}
        2. ESTRUTURA/PADR√ÉO: Como formar e usar
        3. EXEMPLOS CONTEXTUAIS: 4-5 exemplos usando o vocabul√°rio dispon√≠vel
        4. PADR√ïES DE USO: Quando e como aplicar
        5. NOTAS VARIANTE: Diferen√ßas {request.variant} se relevante
        
        Mantenha foco pedag√≥gico sistem√°tico e use vocabul√°rio dispon√≠vel nos exemplos."""
        
        return [
            SystemMessage(content=system_prompt),
            HumanMessage(content=human_prompt)
        ]

    async def _generate_l1_prevention_prompt(
        self, 
        request: GrammarRequest, 
        grammar_point: str
    ) -> List[Any]:
        """Gerar prompt para GRAMMAR 2: Preven√ß√£o L1‚ÜíL2."""
        
        # AN√ÅLISE VIA IA: Padr√µes de interfer√™ncia L1
        l1_analysis = await self._analyze_l1_interference_ai(
            grammar_point=grammar_point,
            context=request.unit_context,
            vocabulary=request.vocabulary_list,
            level=request.level
        )
        
        system_prompt = f"""Voc√™ √© um especialista em interfer√™ncia lingu√≠stica portugu√™s‚Üíingl√™s.
        
        Sua tarefa √© focar na preven√ß√£o de erros espec√≠ficos que brasileiros cometem com este ponto gramatical.
        
        ESTRAT√âGIA: GRAMMAR 2 - Preven√ß√£o de Erros L1‚ÜíL2
        AN√ÅLISE L1: {l1_analysis}"""
        
        human_prompt = f"""Crie conte√∫do focado em preven√ß√£o L1‚ÜíL2:
        
        PONTO GRAMATICAL: {grammar_point}
        TEXTO BASE: {request.input_text}
        VOCABUL√ÅRIO: {', '.join(request.vocabulary_list[:10])}
        N√çVEL: {request.level}
        CONTEXTO: {request.unit_context}
        
        FOCO EM INTERFER√äNCIA PORTUGU√äS‚ÜíINGL√äS:
        1. ERRO COMUM: Principal erro que brasileiros cometem
        2. PADR√ÉO PORTUGU√äS: Como seria em portugu√™s
        3. FORMA INCORRETA: Como brasileiros falam/escrevem erroneamente
        4. FORMA CORRETA: Como deve ser em ingl√™s
        5. EXEMPLOS CONTRASTIVOS: Portugu√™s vs Ingl√™s correto
        6. ESTRAT√âGIAS DE PREVEN√á√ÉO: Como evitar o erro
        
        Use vocabul√°rio dispon√≠vel e contexto "{request.unit_context}" nos exemplos."""
        
        return [
            SystemMessage(content=system_prompt),
            HumanMessage(content=human_prompt)
        ]

    async def _analyze_systematic_approach_ai(
        self, 
        grammar_point: str, 
        level: str, 
        context: str, 
        vocabulary: List[str]
    ) -> str:
        """An√°lise via IA da melhor abordagem sistem√°tica."""
        
        system_prompt = """Voc√™ √© um especialista em metodologia de ensino de gram√°tica.
        
        Determine a melhor abordagem sistem√°tica para ensinar este ponto gramatical considerando o contexto espec√≠fico."""
        
        human_prompt = f"""Determine abordagem sistem√°tica para:
        
        PONTO GRAMATICAL: {grammar_point}
        N√çVEL: {level}
        CONTEXTO: {context}
        VOCABUL√ÅRIO DISPON√çVEL: {', '.join(vocabulary[:8])}
        
        Recomende a abordagem pedag√≥gica mais eficaz:
        - Sequ√™ncia de apresenta√ß√£o
        - Foco principal para este n√≠vel
        - Como usar o vocabul√°rio dispon√≠vel
        - Progress√£o l√≥gica de conceitos
        
        Retorne estrat√©gia sistem√°tica espec√≠fica para este contexto."""
        
        try:
            messages = [
                SystemMessage(content=system_prompt),
                HumanMessage(content=human_prompt)
            ]
            
            response = await self.llm.ainvoke(messages)
            return response.content.strip()
            
        except Exception as e:
            logger.warning(f"Erro na an√°lise sistem√°tica via IA: {str(e)}")
            return f"Abordagem sistem√°tica adaptada para {level} no contexto {context}"

    async def _analyze_l1_interference_ai(
        self, 
        grammar_point: str, 
        context: str, 
        vocabulary: List[str], 
        level: str
    ) -> str:
        """An√°lise via IA de padr√µes de interfer√™ncia L1 (portugu√™s‚Üíingl√™s)."""
        
        system_prompt = """Voc√™ √© um especialista em interfer√™ncia lingu√≠stica portugu√™s-ingl√™s.
        
        Analise os principais erros que brasileiros cometem com este ponto gramatical espec√≠fico."""
        
        human_prompt = f"""Analise interfer√™ncia L1‚ÜíL2 para:
        
        PONTO GRAMATICAL: {grammar_point}
        CONTEXTO: {context}
        VOCABUL√ÅRIO: {', '.join(vocabulary[:8])}
        N√çVEL: {level}
        
        Identifique:
        - Principal erro que brasileiros cometem com {grammar_point}
        - Por que este erro acontece (influ√™ncia do portugu√™s)
        - Padr√µes espec√≠ficos de interfer√™ncia neste contexto
        - Exemplos t√≠picos de erro portugu√™s‚Üíingl√™s
        
        Foque nos erros mais comuns para {level} no contexto "{context}".
        
        Retorne an√°lise espec√≠fica de interfer√™ncia L1."""
        
        try:
            messages = [
                SystemMessage(content=system_prompt),
                HumanMessage(content=human_prompt)
            ]
            
            response = await self.llm.ainvoke(messages)
            return response.content.strip()
            
        except Exception as e:
            logger.warning(f"Erro na an√°lise L1 via IA: {str(e)}")
            return f"An√°lise de interfer√™ncia L1 para {grammar_point} no contexto {context}"

    async def _parse_grammar_response_ai(
        self, 
        content: str, 
        request: GrammarRequest,
        grammar_point: str
    ) -> GrammarContent:
        """Parser inteligente via IA para estruturar resposta."""
        
        system_prompt = """Voc√™ √© um especialista em estrutura√ß√£o de conte√∫do educacional.
        
        Extraia e organize as informa√ß√µes da resposta em categorias estruturadas."""
        
        human_prompt = f"""Estruture este conte√∫do gramatical:
        
        CONTE√öDO: {content}
        ESTRAT√âGIA: {request.strategy}
        PONTO GRAMATICAL: {grammar_point}
        
        Extraia e organize em formato JSON:
        {{
            "explanation": "explica√ß√£o principal clara",
            "examples": ["exemplo 1", "exemplo 2", "exemplo 3"],
            "patterns": ["padr√£o 1", "padr√£o 2"],
            "variant_notes": "notas sobre variante se houver",
            "l1_focus": "aspectos de interfer√™ncia L1 se aplic√°vel"
        }}
        
        Mantenha foco na estrat√©gia {request.strategy}."""
        
        try:
            messages = [
                SystemMessage(content=system_prompt),
                HumanMessage(content=human_prompt)
            ]
            
            response = await self.llm.ainvoke(messages)
            
            # Tentar parsear JSON
            try:
                if "```json" in response.content:
                    json_content = response.content.split("```json")[1].split("```")[0].strip()
                else:
                    json_content = response.content
                
                parsed_data = json.loads(json_content)
                
                # Determinar estrat√©gia
                strategy = GrammarStrategy.EXPLICACAO_SISTEMATICA
                if request.strategy == "l1_prevention":
                    strategy = GrammarStrategy.PREVENCAO_ERROS_L1
                
                return GrammarContent(
                    strategy=strategy,
                    grammar_point=grammar_point,
                    systematic_explanation=parsed_data.get("explanation", "Explica√ß√£o via IA"),
                    usage_rules=parsed_data.get("patterns", [f"Regra principal de {grammar_point}"]),
                    examples=parsed_data.get("examples", [f"Example with {grammar_point}"]),
                    l1_interference_notes=[parsed_data.get("l1_focus", f"Poss√≠vel interfer√™ncia L1 com {grammar_point}")],
                    common_mistakes=[{
                        "mistake": f"Common error with {grammar_point}",
                        "correction": "Correct form",
                        "explanation": "Explanation of error"
                    }],
                    vocabulary_integration=[],
                    previous_grammar_connections=[],
                    selection_rationale=f"Estrat√©gia {request.strategy} via fallback"
                )
                
            except (json.JSONDecodeError, KeyError):
                logger.warning("Erro no parsing JSON, usando parser t√©cnico fallback")
                return self._technical_parser_fallback(content, request, grammar_point)
                
        except Exception as e:
            logger.warning(f"Erro no parser IA: {str(e)}")
            return self._technical_parser_fallback(content, request, grammar_point)

    # =============================================================================
    # PARSER T√âCNICO (MANTIDO - UTILIT√ÅRIO T√âCNICO)
    # =============================================================================

    def _technical_parser_fallback(
        self, 
        content: str, 
        request: GrammarRequest, 
        grammar_point: str
    ) -> GrammarContent:
        """Parser t√©cnico fallback quando IA parsing falha."""
        
        lines = [line.strip() for line in content.split('\n') if line.strip()]
        
        # Inicializar campos
        explanation = ""
        examples = []
        patterns = []
        variant_notes = None
        l1_interference_focus = None
        
        current_section = None
        
        # Parsing contextual t√©cnico
        for line in lines:
            line_lower = line.lower()
            
            # Detectar se√ß√µes por palavras-chave t√©cnicas
            if any(kw in line_lower for kw in ["explica√ß√£o", "explanation", "regra"]):
                current_section = "explanation"
                if ":" in line:
                    explanation += line.split(":", 1)[-1].strip() + " "
                    
            elif any(kw in line_lower for kw in ["exemplos", "examples"]):
                current_section = "examples"
                
            elif any(kw in line_lower for kw in ["padr√µes", "patterns", "uso"]):
                current_section = "patterns"
                
            elif any(kw in line_lower for kw in ["variante", "variant", "diferen√ßas"]):
                current_section = "variant"
                
            elif any(kw in line_lower for kw in ["erro", "interfer√™ncia", "l1", "portugu√™s"]):
                current_section = "l1"
                
            else:
                # Adicionar conte√∫do √† se√ß√£o atual (l√≥gica t√©cnica)
                if current_section == "explanation":
                    explanation += line + " "
                elif current_section == "examples":
                    if line.startswith(("‚Ä¢", "-", "1.", "2.", "3.", "*")) or len(line) > 20:
                        examples.append(line.lstrip("‚Ä¢-123456789.*‚Ä¢ "))
                elif current_section == "patterns":
                    if line.startswith(("‚Ä¢", "-", "1.", "2.", "3.", "*")):
                        patterns.append(line.lstrip("‚Ä¢-123456789.*‚Ä¢ "))
                elif current_section == "variant":
                    variant_notes = (variant_notes or "") + line + " "
                elif current_section == "l1":
                    l1_interference_focus = {"focus": line}
        
        # Fallbacks t√©cnicos
        if not explanation:
            explanation = content[:300].strip()
        if not examples:
            sentences = [s.strip() for s in content.replace('\n', ' ').split('.') 
                        if 15 < len(s.strip()) < 100]
            examples = sentences[:3] if sentences else ["Exemplo contextual"]
        if not patterns:
            patterns = ["Padr√£o gramatical identificado"]
        
        # Determinar estrat√©gia
        strategy = GrammarStrategy.EXPLICACAO_SISTEMATICA
        if request.strategy == "l1_prevention":
            strategy = GrammarStrategy.PREVENCAO_ERROS_L1
        
        return GrammarContent(
            strategy=strategy,
            grammar_point=grammar_point,
            systematic_explanation=explanation.strip(),
            usage_rules=patterns[:3] if patterns else [f"Regra principal de {grammar_point}"],
            examples=examples[:5] if examples else [f"Example with {grammar_point}"],
            l1_interference_notes=[l1_interference_focus.get("focus", f"Interfer√™ncia L1 com {grammar_point}") if l1_interference_focus else f"Interfer√™ncia L1 com {grammar_point}"],
            common_mistakes=[{
                "mistake": f"Common error with {grammar_point}",
                "correction": "Correct form", 
                "explanation": "Technical fallback error"
            }],
            vocabulary_integration=[],
            previous_grammar_connections=[],
            selection_rationale=f"Estrat√©gia {request.strategy} via parser t√©cnico"
        )

    # =============================================================================
    # UTILIT√ÅRIOS T√âCNICOS (MANTIDOS)
    # =============================================================================

    def format_for_output(self, grammar_content: GrammarContent) -> Dict[str, Any]:
        """Formatar para sa√≠da estruturada (utilit√°rio t√©cnico)."""
        return {
            "type": "grammar",
            "grammar_point": grammar_content.grammar_point,
            "explanation": grammar_content.systematic_explanation,
            "examples": grammar_content.examples,
            "usage_rules": grammar_content.usage_rules,
            "strategy": str(grammar_content.strategy),
            "l1_interference_notes": grammar_content.l1_interference_notes,
            "metadata": {
                "generated_at": "timestamp",
                "section": "grammar",
                "langchain_version": "0.3.x",
                "pydantic_version": "2.x",
                "ai_contextual": True
            }
        }

    async def get_service_status(self) -> Dict[str, Any]:
        """Status do servi√ßo (utilit√°rio t√©cnico)."""
        return {
            "service": "GrammarGenerator",
            "status": "active",
            "strategies": list(GRAMMAR_STRATEGIES.values()),
            "ai_integration": "100% contextual analysis",
            "langchain_version": "0.3.x",
            "supported_levels": CEFR_LEVELS,
            "supported_variants": LANGUAGE_VARIANTS,
            "ai_methods": [
                "_identify_grammar_point_ai",
                "_analyze_systematic_approach_ai",
                "_analyze_l1_interference_ai", 
                "_parse_grammar_response_ai"
            ]
        }


# üöÄ Fun√ß√£o utilit√°ria moderna (MANTIDA - INTERFACE T√âCNICA)
async def generate_grammar(
    text: str, 
    vocabulary: List[str], 
    level: str = "B1", 
    variant: str = "american",
    unit_context: str = "",
    strategy: str = "systematic"
) -> Dict[str, Any]:
    """
    Fun√ß√£o simplificada para gerar gram√°tica com LangChain 0.3 + IA contextual.
    
    Args:
        text: Texto base
        vocabulary: Lista de vocabul√°rio
        level: N√≠vel CEFR
        variant: Variante do ingl√™s
        unit_context: Contexto espec√≠fico da unidade
        strategy: "systematic" ou "l1_prevention"
        
    Returns:
        Dict: Conte√∫do gramatical formatado
    """
    generator = GrammarGenerator()
    
    # Pydantic 2 - valida√ß√£o autom√°tica
    request = GrammarRequest(
        input_text=text,
        vocabulary_list=vocabulary,
        level=level,
        variant=variant,
        unit_context=unit_context,
        strategy=strategy
    )
    
    grammar_content = await generator.generate_grammar_content(request)
    return generator.format_for_output(grammar_content)

# =============================================================================
# GRAMMAR GENERATOR SERVICE - CLASSE FALTANTE
# =============================================================================

class GrammarGeneratorService:
    """
    Service wrapper para GrammarGenerator - compatibilidade com sistema IVO V2.
    Esta classe fornece interface compat√≠vel esperada pelo sistema.
    """
    
    def __init__(self):
        """Inicializar service com generator interno."""
        self.generator = GrammarGenerator()
        logger.info("‚úÖ GrammarGeneratorService inicializado")
    
    async def generate_grammar_content(
        self, 
        request: GrammarRequest
    ) -> GrammarContent:
        """
        Interface principal para gera√ß√£o de gram√°tica.
        
        Args:
            request: Dados da requisi√ß√£o validados pelo Pydantic 2
            
        Returns:
            GrammarContent: Conte√∫do estruturado por estrat√©gia
        """
        return await self.generator.generate_grammar_content(request)
    
    async def generate_grammar_for_unit(
        self,
        unit_data: Dict[str, Any],
        vocabulary_items: List[str],
        context: str = "",
        strategy: str = "systematic"
    ) -> GrammarContent:
        """
        Gerar gram√°tica para uma unidade espec√≠fica.
        Interface compat√≠vel com sistema hier√°rquico IVO V2.
        
        Args:
            unit_data: Dados da unidade
            vocabulary_items: Vocabul√°rio da unidade
            context: Contexto da unidade
            strategy: Estrat√©gia gramatical
            
        Returns:
            GrammarContent: Objeto Pydantic com conte√∫do gramatical para unidade
        """
        try:
            # Extrair informa√ß√µes da unidade
            level = unit_data.get("cefr_level", "B1")
            variant = unit_data.get("language_variant", "american_english").replace("_english", "")
            unit_context = context or unit_data.get("context", "")
            
            # Texto base da unidade
            text_sources = []
            if unit_data.get("main_aim"):
                text_sources.append(unit_data["main_aim"])
            if unit_data.get("subsidiary_aims"):
                text_sources.extend(unit_data["subsidiary_aims"])
            if unit_context:
                text_sources.append(unit_context)
            
            input_text = ". ".join(text_sources) or "Grammar practice with available vocabulary."
            
            # Criar request
            request = GrammarRequest(
                input_text=input_text,
                vocabulary_list=vocabulary_items,
                level=level,
                variant=variant,
                unit_context=unit_context,
                strategy=strategy
            )
            
            # Gerar conte√∫do
            grammar_content = await self.generator.generate_grammar_content(request)
            
            # Adicionar metadados diretamente no objeto (se necess√°rio)
            # Os metadados de unidade podem ser adicionados depois na API
            
            logger.info(f"‚úÖ Gram√°tica gerada para unidade: {grammar_content.grammar_point}")
            return grammar_content
            
        except Exception as e:
            logger.error(f"‚ùå Erro na gera√ß√£o de gram√°tica para unidade: {str(e)}")
            raise
    
    def get_available_strategies(self) -> List[str]:
        """Retornar estrat√©gias dispon√≠veis."""
        return list(GRAMMAR_STRATEGIES.keys())
    
    async def get_service_status(self) -> Dict[str, Any]:
        """Status do service."""
        generator_status = await self.generator.get_service_status()
        return {
            **generator_status,
            "service_name": "GrammarGeneratorService",
            "wrapper_status": "active",
            "ivo_v2_compatible": True
        }
    
    async def validate_grammar_request(self, request_data: Dict[str, Any]) -> bool:
        """Validar dados de requisi√ß√£o."""
        try:
            GrammarRequest(**request_data)
            return True
        except ValidationError:
            return False


# =============================================================================
# INST√ÇNCIA GLOBAL PARA COMPATIBILIDADE
# =============================================================================

# Inst√¢ncia global que ser√° importada pelo sistema
grammar_service = GrammarGeneratorService()


# =============================================================================
# FUN√á√ÉO DE CONVENI√äNCIA PARA INTEGRA√á√ÉO
# =============================================================================

async def create_grammar_for_unit(
    unit_data: Dict[str, Any],
    vocabulary_items: List[str] = None,
    context: str = "",
    strategy: str = "systematic"
) -> Dict[str, Any]:
    """
    Fun√ß√£o de conveni√™ncia para integra√ß√£o com sistema IVO V2.
    
    Args:
        unit_data: Dados completos da unidade
        vocabulary_items: Lista de vocabul√°rio (opcional)
        context: Contexto adicional
        strategy: "systematic" ou "l1_prevention"
        
    Returns:
        Dict: Conte√∫do gramatical pronto para uso
    """
    service = GrammarGeneratorService()
    
    # Usar vocabul√°rio da unidade se n√£o fornecido
    if vocabulary_items is None:
        vocabulary_section = unit_data.get("vocabulary", {})
        if isinstance(vocabulary_section, dict):
            items = vocabulary_section.get("items", [])
            vocabulary_items = [item.get("word", "") for item in items if isinstance(item, dict)]
        else:
            vocabulary_items = []
    
    return await service.generate_grammar_for_unit(
        unit_data=unit_data,
        vocabulary_items=vocabulary_items,
        context=context,
        strategy=strategy
    )

# Exemplo e teste para LangChain 0.3 + IA Contextual
if __name__ == "__main__":
    async def test_langchain_v03_ai():
        """Testar LangChain 0.3 + IA contextual."""
        try:
            print("üß™ Testando LangChain 0.3 + IA Contextual...")
            
            # Testar GRAMMAR 1: Sistem√°tica
            result1 = await generate_grammar(
                text="The students have been learning English for two years.",
                vocabulary=["learn", "student", "year", "English", "study"],
                level="B2",
                variant="american",
                unit_context="academic environment",
                strategy="systematic"
            )
            
            print("üéØ GRAMMAR 1 (Sistem√°tica):")
            print(f"   Ponto: {result1['grammar_point']}")
            print(f"   Estrat√©gia: {result1['strategy_type']}")
            
            # Testar GRAMMAR 2: L1 Prevention
            result2 = await generate_grammar(
                text="I am working here since 2020.",
                vocabulary=["work", "since", "here", "year"],
                level="A2", 
                variant="american",
                unit_context="workplace",
                strategy="l1_prevention"
            )
            
            print("üéØ GRAMMAR 2 (L1 Prevention):")
            print(f"   Ponto: {result2['grammar_point']}")
            print(f"   Estrat√©gia: {result2['strategy_type']}")
            print(f"   L1 Focus: {result2['l1_interference_focus']}")
            
            print("üéâ LangChain 0.3 + IA Contextual funcionando!")
            
        except Exception as e:
            print(f"‚ùå Erro no teste: {e}")
    
    # Executar teste
    asyncio.run(test_langchain_v03_ai())