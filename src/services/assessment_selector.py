# src/services/assessment_selector.py
"""
Servi√ßo de sele√ß√£o inteligente de atividades de avalia√ß√£o para o IVO V2.
Implementa balanceamento RAG das 7 atividades dispon√≠veis com sele√ß√£o via IA contextual.
CORRIGIDO: 100% an√°lise via IA, zero hard-coded, balanceamento inteligente hier√°rquico.
"""

import asyncio
import json
import logging
import time
from typing import Dict, List, Any, Optional, Tuple
from datetime import datetime

from langchain_openai import ChatOpenAI
from langchain.schema import SystemMessage, HumanMessage
from pydantic import BaseModel, ValidationError, Field, ConfigDict

from src.core.unit_models import AssessmentActivity, AssessmentSection, AssessmentGenerationRequest
from src.core.enums import CEFRLevel, LanguageVariant, UnitType, AssessmentType
from config.models import get_openai_config, load_model_configs

logger = logging.getLogger(__name__)


# =============================================================================
# CONSTANTES T√âCNICAS (MANTIDAS - METODOLOGIA ESTABELECIDA)
# =============================================================================

ASSESSMENT_TYPES = [
    "cloze_test",      # 1. Compreens√£o geral com lacunas
    "gap_fill",        # 2. Lacunas espec√≠ficas de vocabul√°rio
    "reordering",      # 3. Reordenar frases/palavras
    "transformation",  # 4. Transformar estruturas gramaticais
    "multiple_choice", # 5. Quest√µes objetivas
    "true_false",      # 6. Afirma√ß√µes verdadeiro/falso
    "matching"         # 7. Associa√ß√£o de elementos
]

ASSESSMENT_DESCRIPTIONS = {
    "cloze_test": "General comprehension with multiple gaps in contextual text",
    "gap_fill": "Specific gaps focusing on unit vocabulary",
    "reordering": "Reorder words/sentences to form correct structures",
    "transformation": "Transform structures while maintaining meaning",
    "multiple_choice": "Objective questions with alternatives",
    "true_false": "Statements to assess textual comprehension",
    "matching": "Associate related elements (words, definitions, images)"
}

SKILLS_ASSESSED_MAPPING = {
    "cloze_test": ["reading_comprehension", "context_analysis", "grammar_in_context"],
    "gap_fill": ["vocabulary_recognition", "word_formation", "context_application"],
    "reordering": ["sentence_structure", "word_order", "syntax_awareness"],
    "transformation": ["grammatical_equivalence", "paraphrasing", "structural_flexibility"],
    "multiple_choice": ["recognition_skills", "discrimination", "elimination_strategies"],
    "true_false": ["reading_comprehension", "inference", "detail_identification"],
    "matching": ["association_skills", "lexical_relationships", "categorization"]
}

CEFR_ASSESSMENT_PREFERENCES = {
    "A1": ["gap_fill", "matching", "true_false", "multiple_choice"],
    "A2": ["gap_fill", "matching", "true_false", "cloze_test"],
    "B1": ["cloze_test", "transformation", "reordering", "gap_fill"],
    "B2": ["transformation", "cloze_test", "reordering", "multiple_choice"],
    "C1": ["transformation", "cloze_test", "reordering", "gap_fill"],
    "C2": ["transformation", "cloze_test", "reordering", "matching"]
}


class AssessmentSelectorService:
    """Servi√ßo principal para sele√ß√£o inteligente de atividades de avalia√ß√£o com RAG."""
    
    def __init__(self):
        """Inicializar servi√ßo com IA contextual."""
        self.openai_config = get_openai_config()
        self.model_configs = load_model_configs()
        
        # NOVO: Usar model_selector para obter o modelo correto
        from src.services.model_selector import get_llm_config_for_service
        
        # Obter configura√ß√£o espec√≠fica para assessment_selector (TIER-5: o3)
        llm_config = get_llm_config_for_service("assessment_selector")
        self.llm = ChatOpenAI(**llm_config)
        
        logger.info("‚úÖ AssessmentSelectorService inicializado com 7 atividades e IA contextual")

    def _create_balance_analysis_schema(self) -> Dict[str, Any]:
        """Schema for assessment balance analysis using structured output."""
        return {
            "title": "AssessmentBalanceAnalysis",
            "description": "Schema for analyzing assessment activity balance and usage patterns",
            "type": "object",
            "properties": {
                "current_distribution": {
                    "type": "object",
                    "additionalProperties": {
                        "type": "integer",
                        "minimum": 0
                    },
                    "description": "Current count of each assessment type"
                },
                "overused_types": {
                    "type": "array",
                    "items": {
                        "type": "string",
                        "enum": ASSESSMENT_TYPES
                    },
                    "description": "Assessment types used too frequently"
                },
                "underused_types": {
                    "type": "array", 
                    "items": {
                        "type": "string",
                        "enum": ASSESSMENT_TYPES
                    },
                    "description": "Assessment types not used enough"
                },
                "balance_score": {
                    "type": "number",
                    "minimum": 0.0,
                    "maximum": 1.0,
                    "description": "Overall balance score"
                },
                "skill_diversity": {
                    "type": "number",
                    "minimum": 0.0,
                    "maximum": 1.0,
                    "description": "Diversity of skills being assessed"
                },
                "recommendations": {
                    "type": "array",
                    "items": {"type": "string"},
                    "description": "Specific recommendations for improving balance"
                },
                "cefr_appropriateness": {
                    "type": "number",
                    "minimum": 0.0,
                    "maximum": 1.0,
                    "description": "How appropriate current selection is for CEFR level"
                }
            },
            "required": ["current_distribution", "overused_types", "underused_types", "balance_score", "skill_diversity", "recommendations", "cefr_appropriateness"],
            "additionalProperties": False
        }
    
    async def select_optimal_assessments(
        self,
        assessment_request: AssessmentGenerationRequest,
        unit_data: Dict[str, Any],
        content_data: Dict[str, Any],
        hierarchy_context: Dict[str, Any],
        rag_context: Dict[str, Any]
    ) -> AssessmentSection:
        """
        Selecionar 2 atividades complementares usando an√°lise IA + RAG hier√°rquico.
        
        Args:
            assessment_request: Request com configura√ß√µes de gera√ß√£o
            unit_data: Dados da unidade (t√≠tulo, contexto, CEFR, etc.)
            content_data: Dados de conte√∫do (vocabul√°rio, sentences, tips, grammar)
            hierarchy_context: Contexto hier√°rquico (curso, book, sequ√™ncia)
            rag_context: Contexto RAG (assessments usados, balanceamento)
            
        Returns:
            AssessmentSection com 2 atividades selecionadas e balanceadas
        """
        try:
            start_time = time.time()
            
            logger.info(f"üéØ Selecionando atividades para unidade {unit_data.get('title', 'Unknown')}")
            logger.info(f"üìä Configura√ß√µes: count={assessment_request.assessment_count}, types={assessment_request.preferred_types}")
            logger.info(f"üìä RAG Context: {len(rag_context.get('used_assessments', []))} assessments anteriores")
            
            # 1. Construir contexto pedag√≥gico enriquecido
            enriched_context = await self._build_assessment_context(
                unit_data, content_data, hierarchy_context, rag_context, assessment_request
            )
            
            # 2. AN√ÅLISE VIA IA: An√°lise de balanceamento atual
            balance_analysis = await self._analyze_current_balance_ai(enriched_context)
            
            # 3. AN√ÅLISE VIA IA: Identificar tipos subutilizados
            underused_types = await self._identify_underused_assessments_ai(
                enriched_context, balance_analysis
            )
            
            # 4. AN√ÅLISE VIA IA: Sele√ß√£o inteligente do par complementar
            selected_pair = await self._select_complementary_pair_ai(
                enriched_context, underused_types, balance_analysis
            )
            
            # 5. AN√ÅLISE VIA IA: Gerar atividades espec√≠ficas
            activity_1 = await self._generate_specific_activity_ai(
                selected_pair[0], enriched_context, 1
            )
            activity_2 = await self._generate_specific_activity_ai(
                selected_pair[1], enriched_context, 2
            )
            
            # 6. AN√ÅLISE VIA IA: Justificativa de sele√ß√£o
            selection_rationale = await self._generate_selection_rationale_ai(
                selected_pair, enriched_context, balance_analysis
            )
            
            # 7. AN√ÅLISE VIA IA: An√°lise de complementaridade
            complementarity_analysis = await self._analyze_complementarity_ai(
                activity_1, activity_2, enriched_context
            )
            
            # 8. Calcular m√©tricas de balanceamento
            updated_balance = self._calculate_updated_balance(
                rag_context.get("used_assessments", {}), selected_pair
            )
            
            # 9. Construir AssessmentSection
            assessment_section = AssessmentSection(
                activities=[activity_1, activity_2],
                selection_rationale=selection_rationale,
                total_estimated_time=activity_1.estimated_time + activity_2.estimated_time,
                skills_assessed=list(set(activity_1.skills_assessed + activity_2.skills_assessed)),
                balance_analysis=balance_analysis,
                underused_activities=underused_types,
                complementary_pair=complementarity_analysis.get("is_complementary", True)
            )
            
            selection_time = time.time() - start_time
            
            logger.info(
                f"‚úÖ Atividades selecionadas: {selected_pair[0]} + {selected_pair[1]} em {selection_time:.2f}s"
            )
            
            return assessment_section
            
        except Exception as e:
            logger.error(f"‚ùå Erro na sele√ß√£o de atividades: {str(e)}")
            raise
    
    async def _build_assessment_context(
        self,
        unit_data: Dict[str, Any],
        content_data: Dict[str, Any],
        hierarchy_context: Dict[str, Any],
        rag_context: Dict[str, Any],
        assessment_request: AssessmentGenerationRequest
    ) -> Dict[str, Any]:
        """Construir contexto pedag√≥gico enriquecido para sele√ß√£o."""
        
        # Extrair vocabul√°rio da unidade
        vocabulary_items = []
        vocabulary_count = 0
        if content_data.get("vocabulary") and content_data["vocabulary"].get("items"):
            vocabulary_items = content_data["vocabulary"]["items"]
            vocabulary_count = len(vocabulary_items)
        
        vocabulary_words = [item.get("word", "") for item in vocabulary_items[:12]]
        
        # Extrair estrat√©gias aplicadas
        applied_strategies = []
        strategy_info = ""
        
        if content_data.get("tips"):
            strategy_info = f"TIPS: {content_data['tips'].get('strategy', '')}"
            applied_strategies.append("tips")
        elif content_data.get("grammar"):
            strategy_info = f"GRAMMAR: {content_data['grammar'].get('strategy', '')}"
            applied_strategies.append("grammar")
        
        # Extrair sentences
        sentences_data = content_data.get("sentences", {})
        sentences_count = len(sentences_data.get("sentences", []))
        sample_sentences = [s.get("text", "") for s in sentences_data.get("sentences", [])][:3]
        
        # An√°lise RAG de atividades usadas
        used_assessments = rag_context.get("used_assessments", {})
        if isinstance(used_assessments, list):
            # Converter lista para dict com contagens
            assessment_counts = {}
            for assessment in used_assessments:
                if isinstance(assessment, dict) and "type" in assessment:
                    assessment_type = assessment["type"]
                    assessment_counts[assessment_type] = assessment_counts.get(assessment_type, 0) + 1
                elif isinstance(assessment, str):
                    assessment_counts[assessment] = assessment_counts.get(assessment, 0) + 1
            used_assessments = assessment_counts
        
        enriched_context = {
            "unit_info": {
                "title": unit_data.get("title", ""),
                "context": unit_data.get("context", ""),
                "cefr_level": unit_data.get("cefr_level", "A2"),
                "unit_type": unit_data.get("unit_type", "lexical_unit"),
                "language_variant": unit_data.get("language_variant", "american_english"),
                "main_aim": unit_data.get("main_aim", ""),
                "subsidiary_aims": unit_data.get("subsidiary_aims", [])
            },
            "content_analysis": {
                "vocabulary_count": vocabulary_count,
                "vocabulary_words": vocabulary_words,
                "vocabulary_items": vocabulary_items,
                "sentences_count": sentences_count,
                "sample_sentences": sample_sentences,
                "applied_strategies": applied_strategies,
                "strategy_info": strategy_info,
                "has_tips": bool(content_data.get("tips")),
                "has_grammar": bool(content_data.get("grammar")),
                "content_richness": self._assess_content_richness(content_data)
            },
            "hierarchy_info": {
                "course_name": hierarchy_context.get("course_name", ""),
                "book_name": hierarchy_context.get("book_name", ""),
                "sequence_order": hierarchy_context.get("sequence_order", 1),
                "target_level": hierarchy_context.get("target_level", unit_data.get("cefr_level"))
            },
            "rag_analysis": {
                "used_assessments": used_assessments,
                "assessment_density": sum(used_assessments.values()) if used_assessments else 0,
                "progression_level": rag_context.get("progression_level", "intermediate"),
                "total_units_processed": hierarchy_context.get("sequence_order", 1),
                "underused_threshold": 2  # M√°ximo 2 usos por tipo a cada 7 unidades
            },
            "selection_preferences": {
                "target_activities": 2,
                "max_total_time": 25,  # 25 minutos m√°ximo
                "prefer_complementary": True,
                "balance_skills": True,
                "avoid_overused": True
            }
        }
        
        return enriched_context
    
    # =============================================================================
    # AN√ÅLISES VIA IA (SUBSTITUEM L√ìGICA HARD-CODED)
    # =============================================================================
    
    async def _analyze_current_balance_ai(self, enriched_context: Dict[str, Any]) -> Dict[str, Any]:
        """An√°lise contextual via IA do balanceamento atual de atividades."""
        
        system_prompt = """Voc√™ √© um especialista em balanceamento de atividades de avalia√ß√£o pedag√≥gica.
        
        Analise o uso hist√≥rico de atividades e identifique padr√µes de balanceamento considerando:
        - Distribui√ß√£o de tipos de atividades
        - Progress√£o pedag√≥gica
        - Variedade de habilidades avaliadas
        - Adequa√ß√£o ao n√≠vel CEFR e contexto"""
        
        unit_info = enriched_context["unit_info"]
        rag_analysis = enriched_context["rag_analysis"]
        hierarchy_info = enriched_context["hierarchy_info"]
        
        used_assessments = rag_analysis["used_assessments"]
        total_units = hierarchy_info["sequence_order"]
        
        human_prompt = f"""Analise o balanceamento atual de atividades:
        
        CONTEXTO:
        - Unidade: {unit_info['context']}
        - N√≠vel: {unit_info['cefr_level']}
        - Tipo: {unit_info['unit_type']}
        - Sequ√™ncia: Unidade {total_units} do {hierarchy_info['book_name']}
        
        HIST√ìRICO DE ATIVIDADES USADAS:
        {json.dumps(used_assessments, indent=2)}
        
        TIPOS DISPON√çVEIS:
        1. cloze_test - Compreens√£o geral
        2. gap_fill - Vocabul√°rio espec√≠fico
        3. reordering - Estrutura/ordem
        4. transformation - Equival√™ncia gramatical
        5. multiple_choice - Reconhecimento
        6. true_false - Compreens√£o textual
        7. matching - Associa√ß√£o
        
        AN√ÅLISE REQUERIDA:
        1. Que tipos est√£o sendo overutilizados (>2 usos)?
        2. Que tipos est√£o subutilizados ou n√£o usados?
        3. H√° boa distribui√ß√£o de habilidades avaliadas?
        4. O balanceamento √© apropriado para {unit_info['cefr_level']}?
        
        Retorne an√°lise em formato JSON:
        {{
            "current_distribution": {{"tipo": count, ...}},
            "overused_types": ["tipo1", "tipo2"],
            "underused_types": ["tipo3", "tipo4"],
            "balance_score": 0.75,
            "skill_diversity": 0.80,
            "recommendations": ["recomenda√ß√£o 1", "recomenda√ß√£o 2"]
        }}"""
        
        try:
            # Usar structured output para garantir formato correto
            balance_schema = self._create_balance_analysis_schema()
            structured_llm = self.llm.with_structured_output(balance_schema)
            
            messages = [
                SystemMessage(content=system_prompt),
                HumanMessage(content=human_prompt)
            ]
            
            logger.info("ü§ñ Consultando IA para an√°lise de balanceamento com structured output...")
            balance_analysis = await structured_llm.ainvoke(messages)
            
            # Validar que retornou dict
            if not isinstance(balance_analysis, dict):
                logger.warning("‚ö†Ô∏è Structured output n√£o retornou dict, convertendo...")
                balance_analysis = dict(balance_analysis) if hasattr(balance_analysis, '__dict__') else {}
            
            # Garantir campos obrigat√≥rios
            balance_analysis = self._ensure_balance_analysis_fields(balance_analysis)
            
            logger.info(f"‚úÖ An√°lise de balanceamento via structured output: score {balance_analysis.get('balance_score', 0.7):.2f}")
            return balance_analysis
                
        except Exception as e:
            logger.error(f"‚ùå Erro na an√°lise de balanceamento com structured output: {str(e)}")
            return self._technical_balance_analysis_fallback(enriched_context)

    def _ensure_balance_analysis_fields(self, analysis: Dict[str, Any]) -> Dict[str, Any]:
        """Garantir campos obrigat√≥rios da an√°lise de balanceamento."""
        defaults = {
            "current_distribution": {},
            "overused_types": [],
            "underused_types": [],
            "balance_score": 0.7,
            "skill_diversity": 0.6,
            "recommendations": ["Manter variedade de atividades"],
            "cefr_appropriateness": 0.8
        }
        
        for field, default_value in defaults.items():
            if not analysis.get(field):
                analysis[field] = default_value
        
        return analysis
    
    async def _identify_underused_assessments_ai(
        self, 
        enriched_context: Dict[str, Any], 
        balance_analysis: Dict[str, Any]
    ) -> List[str]:
        """Identificar atividades subutilizadas via an√°lise IA."""
        
        system_prompt = """Voc√™ √© um especialista em identifica√ß√£o de lacunas em atividades pedag√≥gicas.
        
        Identifique quais tipos de atividades precisam ser priorizados para melhor balanceamento."""
        
        unit_info = enriched_context["unit_info"]
        rag_analysis = enriched_context["rag_analysis"]
        
        human_prompt = f"""Identifique atividades subutilizadas:
        
        AN√ÅLISE DE BALANCEAMENTO:
        {json.dumps(balance_analysis, indent=2)}
        
        CONTEXTO ATUAL:
        - N√≠vel: {unit_info['cefr_level']}
        - Tipo: {unit_info['unit_type']}
        - Total de unidades: {rag_analysis.get('total_units_processed', 1)}
        
        TIPOS DISPON√çVEIS: {', '.join(ASSESSMENT_TYPES)}
        
        Considerando:
        1. Que tipos aparecem < 2 vezes no hist√≥rico?
        2. Que tipos s√£o apropriados para {unit_info['cefr_level']}?
        3. Que tipos complementam o tipo de unidade {unit_info['unit_type']}?
        
        Retorne lista de 3-4 tipos priorizados em ordem de necessidade:
        ["tipo_mais_necessario", "tipo_segundo", "tipo_terceiro"]"""
        
        try:
            messages = [
                SystemMessage(content=system_prompt),
                HumanMessage(content=human_prompt)
            ]
            
            response = await self.llm.ainvoke(messages)
            
            # Extrair lista da resposta
            underused = []
            response_lower = response.content.lower()
            
            # Tentar parse JSON primeiro
            try:
                if '[' in response.content and ']' in response.content:
                    list_match = response.content[response.content.find('['):response.content.find(']')+1]
                    underused_list = json.loads(list_match)
                    underused = [item.strip('"') for item in underused_list if item in ASSESSMENT_TYPES]
            except:
                pass
            
            # Fallback: buscar por tipos mencionados
            if not underused:
                for assessment_type in ASSESSMENT_TYPES:
                    if assessment_type in response_lower:
                        underused.append(assessment_type)
            
            # Garantir m√°ximo 4 tipos
            return underused[:4]
            
        except Exception as e:
            logger.warning(f"Erro na identifica√ß√£o de atividades subutilizadas via IA: {str(e)}")
            return self._technical_underused_fallback(enriched_context)
    
    async def _select_complementary_pair_ai(
        self, 
        enriched_context: Dict[str, Any], 
        underused_types: List[str],
        balance_analysis: Dict[str, Any]
    ) -> Tuple[str, str]:
        """Sele√ß√£o inteligente do par complementar via an√°lise IA."""
        
        system_prompt = """Voc√™ √© um especialista em sele√ß√£o de atividades complementares para avalia√ß√£o.
        
        Selecione 2 atividades que se complementem pedagogicamente, considerando:
        - Balanceamento de habilidades avaliadas
        - Complementaridade metodol√≥gica
        - Adequa√ß√£o ao contexto e n√≠vel
        - Prioriza√ß√£o de tipos subutilizados"""
        
        unit_info = enriched_context["unit_info"]
        content_analysis = enriched_context["content_analysis"]
        
        human_prompt = f"""Selecione par complementar de atividades:
        
        CONTEXTO:
        - Unidade: {unit_info['context']}
        - N√≠vel: {unit_info['cefr_level']}
        - Tipo: {unit_info['unit_type']}
        - Vocabul√°rio: {content_analysis['vocabulary_count']} palavras
        - Estrat√©gia: {content_analysis['strategy_info']}
        
        TIPOS PRIORIZADOS (subutilizados): {', '.join(underused_types)}
        
        TODOS OS TIPOS DISPON√çVEIS:
        1. cloze_test - Compreens√£o contextual geral
        2. gap_fill - Vocabul√°rio espec√≠fico
        3. reordering - Estrutura e ordem
        4. transformation - Equival√™ncia gramatical
        5. multiple_choice - Reconhecimento objetivo
        6. true_false - Compreens√£o de afirma√ß√µes
        7. matching - Associa√ß√µes e rela√ß√µes
        
        HABILIDADES POR TIPO:
        - cloze_test: leitura, an√°lise contextual, gram√°tica em contexto
        - gap_fill: vocabul√°rio, forma√ß√£o de palavras, aplica√ß√£o contextual
        - reordering: estrutura sint√°tica, ordem de palavras
        - transformation: equival√™ncia gramatical, par√°frase
        - multiple_choice: discrimina√ß√£o, estrat√©gias de elimina√ß√£o
        - true_false: infer√™ncia, identifica√ß√£o de detalhes
        - matching: associa√ß√£o, rela√ß√µes lexicais
        
        CRIT√âRIOS DE SELE√á√ÉO:
        1. Priorize tipos de {underused_types} se apropriados
        2. Selecione atividades que avaliem habilidades DIFERENTES
        3. Uma deve focar vocabul√°rio, outra estrutura/compreens√£o
        4. Tempo total deve ser 15-25 minutos
        5. Ambas devem ser apropriadas para {unit_info['cefr_level']}
        
        Para unidade {unit_info['unit_type']}:
        - Se lexical: priorize gap_fill, matching, true_false
        - Se gramatical: priorize transformation, reordering, cloze_test
        
        Retorne APENAS os 2 tipos selecionados:
        ["tipo1", "tipo2"]"""
        
        try:
            messages = [
                SystemMessage(content=system_prompt),
                HumanMessage(content=human_prompt)
            ]
            
            response = await self.llm.ainvoke(messages)
            
            # Extrair par da resposta
            selected_pair = []
            response_content = response.content
            
            # Tentar parse JSON
            try:
                if '[' in response_content and ']' in response_content:
                    list_match = response_content[response_content.find('['):response_content.find(']')+1]
                    pair_list = json.loads(list_match)
                    selected_pair = [item.strip('"') for item in pair_list if item in ASSESSMENT_TYPES]
            except:
                pass
            
            # Fallback: identificar tipos mencionados
            if len(selected_pair) < 2:
                found_types = []
                for assessment_type in ASSESSMENT_TYPES:
                    if assessment_type in response_content.lower():
                        found_types.append(assessment_type)
                
                selected_pair = found_types[:2]
            
            # Garantir que temos exatamente 2 tipos
            if len(selected_pair) < 2:
                selected_pair = self._technical_pair_fallback(enriched_context, underused_types)
            
            return tuple(selected_pair[:2])
            
        except Exception as e:
            logger.warning(f"Erro na sele√ß√£o do par via IA: {str(e)}")
            return self._technical_pair_fallback(enriched_context, underused_types)
    
    def _create_assessment_activity_schema(self, activity_type: str, vocabulary_words: List[str]) -> Dict[str, Any]:
        """Create JSON schema for specific AssessmentActivity."""
        return {
            "title": "AssessmentActivity",
            "description": f"Schema for structured {activity_type} assessment activity generation with vocabulary integration and skill evaluation",
            "type": "object",
            "properties": {
                "type": {
                    "type": "string",
                    "enum": [activity_type],
                    "description": f"Activity type: {activity_type}"
                },
                "title": {
                    "type": "string",
                    "description": "Clear and engaging activity title",
                    "minLength": 10,
                    "maxLength": 80
                },
                "instructions": {
                    "type": "string",
                    "description": "Clear instructions for student",
                    "minLength": 20,
                    "maxLength": 300
                },
                "content": {
                    "type": "object",
                    "description": "Specific activity content",
                    "additionalProperties": True
                },
                "answer_key": {
                    "type": "object",
                    "description": "Correct answers",
                    "additionalProperties": True
                },
                "estimated_time": {
                    "type": "integer",
                    "description": "Estimated time in minutes",
                    "minimum": 5,
                    "maximum": 20
                },
                "difficulty_level": {
                    "type": "string",
                    "enum": ["beginner", "intermediate", "advanced"],
                    "description": "Difficulty level"
                },
                "skills_assessed": {
                    "type": "array",
                    "items": {"type": "string"},
                    "description": "Skills assessed"
                },
                "vocabulary_focus": {
                    "type": "array",
                    "items": {"type": "string"},
                    "description": "Focused vocabulary",
                    "maxItems": 5
                },
                "pronunciation_focus": {
                    "type": "boolean",
                    "description": "Whether it focuses on pronunciation"
                },
                "phonetic_elements": {
                    "type": "array",
                    "items": {"type": "string"},
                    "description": "Phonetic elements"
                }
            },
            "required": [
                "type", "title", "instructions", "content", "answer_key",
                "estimated_time", "difficulty_level", "skills_assessed",
                "vocabulary_focus", "pronunciation_focus", "phonetic_elements"
            ],
            "additionalProperties": False
        }
    
    def _clean_assessment_activity_data(self, activity_data: Dict[str, Any], activity_type: str, vocabulary_words: List[str]) -> Dict[str, Any]:
        """Limpar e validar dados da atividade de assessment."""
        cleaned = {
            "type": activity_type,
            "title": str(activity_data.get("title", f"{activity_type.replace('_', ' ').title()} Activity"))[:80],
            "instructions": str(activity_data.get("instructions", f"Complete the {activity_type.replace('_', ' ')} exercise."))[:300],
            "content": activity_data.get("content", {}),
            "answer_key": activity_data.get("answer_key", {}),
            "estimated_time": max(5, min(20, int(activity_data.get("estimated_time", 12)))),
            "difficulty_level": activity_data.get("difficulty_level", "intermediate"),
            "skills_assessed": list(activity_data.get("skills_assessed", SKILLS_ASSESSED_MAPPING.get(activity_type, []))),
            "vocabulary_focus": list(activity_data.get("vocabulary_focus", vocabulary_words[:3]))[:5],
            "pronunciation_focus": bool(activity_data.get("pronunciation_focus", False)),
            "phonetic_elements": list(activity_data.get("phonetic_elements", []))
        }
        
        # Validar difficulty_level
        if cleaned["difficulty_level"] not in ["beginner", "intermediate", "advanced"]:
            cleaned["difficulty_level"] = "intermediate"
        
        # Garantir que content e answer_key s√£o dicts
        if not isinstance(cleaned["content"], dict):
            cleaned["content"] = {}
        if not isinstance(cleaned["answer_key"], dict):
            cleaned["answer_key"] = {}
        
        # Garantir skills_assessed padr√£o se vazio
        if not cleaned["skills_assessed"]:
            cleaned["skills_assessed"] = SKILLS_ASSESSED_MAPPING.get(activity_type, ["general_assessment"])
        
        return cleaned
    
    async def _generate_specific_activity_ai(
        self, 
        activity_type: str, 
        enriched_context: Dict[str, Any],
        activity_number: int
    ) -> AssessmentActivity:
        """Gerar atividade espec√≠fica via an√°lise IA contextual com structured output."""
        
        unit_info = enriched_context["unit_info"]
        content_analysis = enriched_context["content_analysis"]
        vocabulary_words = content_analysis["vocabulary_words"]
        sample_sentences = content_analysis["sample_sentences"]
        
        # Schema para structured output
        activity_schema = self._create_assessment_activity_schema(activity_type, vocabulary_words)
        
        try:
            # Configurar LLM com structured output
            structured_llm = self.llm.with_structured_output(activity_schema)
            
            system_prompt = f"""Voc√™ √© um especialista em cria√ß√£o de atividades de avalia√ß√£o do tipo {activity_type}.
            
            Crie uma atividade espec√≠fica e pr√°tica usando o conte√∫do da unidade.
            
            TIPO: {activity_type}
            DESCRI√á√ÉO: {ASSESSMENT_DESCRIPTIONS[activity_type]}
            HABILIDADES AVALIADAS: {', '.join(SKILLS_ASSESSED_MAPPING[activity_type])}
            
            INSTRU√á√ïES ESPEC√çFICAS:
            {self._get_activity_specific_instructions(activity_type, unit_info['cefr_level'])}
            
            Use vocabul√°rio da unidade e crie conte√∫do espec√≠fico para o contexto."""
            
            human_prompt = f"""Crie atividade {activity_type} para:
            
            CONTEXTO: {unit_info['context']}
            N√çVEL: {unit_info['cefr_level']}
            VOCABUL√ÅRIO: {', '.join(vocabulary_words)}
            SENTENCES DISPON√çVEIS: {'; '.join(sample_sentences)}
            
            A estrutura do content deve seguir:
            {self._get_content_structure_for_type(activity_type)}
            
            Use vocabul√°rio espec√≠fico da unidade e adapte para o n√≠vel {unit_info['cefr_level']}."""
            
            messages = [
                SystemMessage(content=system_prompt),
                HumanMessage(content=human_prompt)
            ]
            
            # Gerar com structured output
            activity_data = await structured_llm.ainvoke(messages)
            
            # Validar e limpar dados
            cleaned_data = self._clean_assessment_activity_data(activity_data, activity_type, vocabulary_words)
            
            # Criar AssessmentActivity
            activity = AssessmentActivity(
                type=AssessmentType(cleaned_data["type"]),
                title=cleaned_data["title"],
                instructions=cleaned_data["instructions"],
                content=cleaned_data["content"],
                answer_key=cleaned_data["answer_key"],
                estimated_time=cleaned_data["estimated_time"],
                difficulty_level=cleaned_data["difficulty_level"],
                skills_assessed=cleaned_data["skills_assessed"],
                vocabulary_focus=cleaned_data["vocabulary_focus"],
                pronunciation_focus=cleaned_data["pronunciation_focus"],
                phonetic_elements=cleaned_data["phonetic_elements"]
            )
            
            logger.info(f"‚úÖ Atividade {activity_type} gerada via structured output: {activity.title}")
            return activity
            
        except Exception as e:
            logger.warning(f"Erro na gera√ß√£o da atividade {activity_type} via structured output: {str(e)}")
            return self._technical_activity_fallback(activity_type, enriched_context, activity_number)
    
    async def _generate_selection_rationale_ai(
        self, 
        selected_pair: Tuple[str, str], 
        enriched_context: Dict[str, Any],
        balance_analysis: Dict[str, Any]
    ) -> str:
        """Gerar justificativa da sele√ß√£o via an√°lise IA."""
        
        system_prompt = """Voc√™ √© um especialista em justifica√ß√£o pedag√≥gica de sele√ß√£o de atividades.
        
        Explique de forma clara e concisa por que estas 2 atividades foram selecionadas considerando o contexto espec√≠fico."""
        
        unit_info = enriched_context["unit_info"]
        content_analysis = enriched_context["content_analysis"]
        
        human_prompt = f"""Justifique a sele√ß√£o das atividades {selected_pair[0]} e {selected_pair[1]}:
        
        CONTEXTO:
        - Unidade: {unit_info['context']}
        - N√≠vel: {unit_info['cefr_level']}
        - Tipo: {unit_info['unit_type']}
        - Vocabul√°rio: {content_analysis['vocabulary_count']} palavras
        - Estrat√©gia: {content_analysis['strategy_info']}
        
        BALANCEAMENTO:
        - Score atual: {balance_analysis.get('balance_score', 0.7):.2f}
        - Tipos subutilizados: {', '.join(balance_analysis.get('underused_types', []))}
        
        ATIVIDADES SELECIONADAS:
        1. {selected_pair[0]} - {ASSESSMENT_DESCRIPTIONS[selected_pair[0]]}
        2. {selected_pair[1]} - {ASSESSMENT_DESCRIPTIONS[selected_pair[1]]}
        
        HABILIDADES AVALIADAS:
        - {selected_pair[0]}: {', '.join(SKILLS_ASSESSED_MAPPING[selected_pair[0]])}
        - {selected_pair[1]}: {', '.join(SKILLS_ASSESSED_MAPPING[selected_pair[1]])}
        
        Explique:
        1. Por que estas atividades s√£o complementares
        2. Como atendem ao balanceamento necess√°rio
        3. Por que s√£o apropriadas para {unit_info['cefr_level']} e "{unit_info['context']}"
        4. Como contribuem para variedade pedag√≥gica
        
        M√°ximo 3-4 frases, seja espec√≠fico e pedag√≥gico."""
        
        try:
            messages = [
                SystemMessage(content=system_prompt),
                HumanMessage(content=human_prompt)
            ]
            
            response = await self.llm.ainvoke(messages)
            return response.content.strip()
            
        except Exception as e:
            logger.warning(f"Erro na gera√ß√£o de justificativa via IA: {str(e)}")
            return f"Atividades {selected_pair[0]} e {selected_pair[1]} selecionadas para balanceamento pedag√≥gico e complementaridade de habilidades no contexto {unit_info['context']}."
    
    async def _analyze_complementarity_ai(
        self, 
        activity_1: AssessmentActivity, 
        activity_2: AssessmentActivity,
        enriched_context: Dict[str, Any]
    ) -> Dict[str, Any]:
        """An√°lise de complementaridade entre as atividades via IA."""
        
        system_prompt = """Voc√™ √© um especialista em an√°lise de complementaridade pedag√≥gica.
        
        Analise se as duas atividades se complementam adequadamente em termos de habilidades e metodologia."""
        
        human_prompt = f"""Analise a complementaridade entre estas atividades:
        
        ATIVIDADE 1:
        - Tipo: {activity_1.type.value if hasattr(activity_1.type, 'value') else activity_1.type}
        - Habilidades: {', '.join(activity_1.skills_assessed)}
        - Tempo: {activity_1.estimated_time} min
        - Foco: {', '.join(activity_1.vocabulary_focus[:3])}
        
        ATIVIDADE 2:
        - Tipo: {activity_2.type.value if hasattr(activity_2.type, 'value') else activity_2.type}
        - Habilidades: {', '.join(activity_2.skills_assessed)}
        - Tempo: {activity_2.estimated_time} min
        - Foco: {', '.join(activity_2.vocabulary_focus[:3])}
        
        Analise:
        1. As habilidades avaliadas s√£o complementares?
        2. H√° sobreposi√ß√£o excessiva ou lacunas?
        3. O tempo total √© apropriado (15-25 min)?
        4. As atividades cobrem aspectos diferentes do aprendizado?
        
        Retorne an√°lise em JSON:
        {{
            "is_complementary": true,
            "skill_overlap": 0.3,
            "skill_coverage": 0.85,
            "time_appropriateness": 0.9,
            "complementarity_score": 0.8,
            "strengths": ["for√ßa 1", "for√ßa 2"],
            "potential_gaps": ["lacuna 1"] 
        }}"""
        
        try:
            messages = [
                SystemMessage(content=system_prompt),
                HumanMessage(content=human_prompt)
            ]
            
            response = await self.llm.ainvoke(messages)
            
            # Parse JSON
            try:
                if "```json" in response.content:
                    json_content = response.content.split("```json")[1].split("```")[0].strip()
                else:
                    json_content = response.content
                
                complementarity = json.loads(json_content)
                return complementarity
                
            except json.JSONDecodeError:
                logger.warning("Erro no parsing da an√°lise de complementaridade")
                return self._technical_complementarity_fallback(activity_1, activity_2)
                
        except Exception as e:
            logger.warning(f"Erro na an√°lise de complementaridade via IA: {str(e)}")
            return self._technical_complementarity_fallback(activity_1, activity_2)
    
    # =============================================================================
    # HELPER METHODS - INSTRU√á√ïES E ESTRUTURAS POR TIPO
    # =============================================================================
    
    def _get_activity_specific_instructions(self, activity_type: str, cefr_level: str) -> str:
        """Obter instru√ß√µes espec√≠ficas por tipo de atividade."""
        
        instructions = {
            "cloze_test": f"""
            - Crie um texto de 80-120 palavras sobre o contexto
            - Remova 8-12 palavras estrat√©gicas (vocabul√°rio + gram√°tica)
            - Lacunas devem testar compreens√£o contextual
            - Para {cefr_level}: ajuste complexidade do texto""",
            
            "gap_fill": f"""
            - Crie 6-8 frases usando vocabul√°rio da unidade
            - Uma lacuna por frase focando palavra espec√≠fica
            - Forne√ßa 3-4 op√ß√µes por lacuna
            - Para {cefr_level}: ajuste complexidade das frases""",
            
            "reordering": f"""
            - Crie 5-6 frases embaralhadas
            - Use vocabul√°rio da unidade
            - Misture ordem de palavras ou frases
            - Para {cefr_level}: ajuste complexidade estrutural""",
            
            "transformation": f"""
            - Crie 5-6 pares de transforma√ß√£o
            - Mantenha significado, mude estrutura
            - Use vocabul√°rio da unidade
            - Para {cefr_level}: ajuste complexidade gramatical""",
            
            "multiple_choice": f"""
            - Crie 6-8 quest√µes objetivas
            - 4 alternativas por quest√£o
            - Foque vocabul√°rio e compreens√£o
            - Para {cefr_level}: ajuste complexidade das alternativas""",
            
            "true_false": f"""
            - Crie texto base de 60-80 palavras
            - 6-8 afirma√ß√µes para julgar
            - Mix de verdadeiro/falso balanceado
            - Para {cefr_level}: ajuste complexidade do racioc√≠nio""",
            
            "matching": f"""
            - Crie 6-8 pares para associar
            - Palavras‚Üîdefini√ß√µes, frases‚Üîcontextos, etc.
            - Use vocabul√°rio da unidade
            - Para {cefr_level}: ajuste complexidade das associa√ß√µes"""
        }
        
        return instructions.get(activity_type, "Crie atividade apropriada para o tipo especificado.")
    
    def _get_content_structure_for_type(self, activity_type: str) -> str:
        """Obter estrutura de conte√∫do por tipo de atividade."""
        
        structures = {
            "cloze_test": '"text": "Texto com _____ lacunas", "gaps": ["palavra1", "palavra2"]',
            "gap_fill": '"sentences": ["Frase com _____"], "options": [["opt1", "opt2", "opt3"]]',
            "reordering": '"scrambled_items": ["palavra/frase embaralhada"], "correct_order": [1, 3, 2]',
            "transformation": '"transformations": [{"original": "frase original", "target": "frase alvo"}]',
            "multiple_choice": '"questions": [{"question": "pergunta", "options": ["a", "b", "c", "d"]}]',
            "true_false": '"text": "texto base", "statements": ["afirma√ß√£o 1", "afirma√ß√£o 2"]',
            "matching": '"left_column": ["item1", "item2"], "right_column": ["match1", "match2"]'
        }
        
        return structures.get(activity_type, '"content": "estrutura espec√≠fica"')
    
    # =============================================================================
    # FALLBACKS T√âCNICOS
    # =============================================================================
    
    def _technical_balance_analysis_fallback(self, enriched_context: Dict[str, Any]) -> Dict[str, Any]:
        """An√°lise de balanceamento t√©cnica quando IA falha."""
        
        used_assessments = enriched_context["rag_analysis"]["used_assessments"]
        
        # An√°lise t√©cnica simples
        total_uses = sum(used_assessments.values()) if used_assessments else 0
        overused = [k for k, v in used_assessments.items() if v > 2]
        underused = [t for t in ASSESSMENT_TYPES if used_assessments.get(t, 0) < 2]
        
        balance_score = 1.0 - (len(overused) * 0.2)  # Penalizar overuso
        skill_diversity = len(set(used_assessments.keys())) / len(ASSESSMENT_TYPES)
        
        return {
            "current_distribution": used_assessments,
            "overused_types": overused,
            "underused_types": underused,
            "balance_score": max(0.3, balance_score),
            "skill_diversity": skill_diversity,
            "recommendations": ["Usar tipos subutilizados", "Evitar overuso"]
        }
    
    def _technical_underused_fallback(self, enriched_context: Dict[str, Any]) -> List[str]:
        """Identifica√ß√£o t√©cnica de atividades subutilizadas."""
        
        used_assessments = enriched_context["rag_analysis"]["used_assessments"]
        cefr_level = enriched_context["unit_info"]["cefr_level"]
        
        # Filtrar por prefer√™ncias CEFR
        preferred_for_level = CEFR_ASSESSMENT_PREFERENCES.get(cefr_level, ASSESSMENT_TYPES[:4])
        
        # Encontrar subutilizados
        underused = []
        for assessment_type in preferred_for_level:
            if used_assessments.get(assessment_type, 0) < 2:
                underused.append(assessment_type)
        
        # Adicionar outros se necess√°rio
        for assessment_type in ASSESSMENT_TYPES:
            if (assessment_type not in underused and 
                used_assessments.get(assessment_type, 0) == 0):
                underused.append(assessment_type)
        
        return underused[:4]
    
    def _technical_pair_fallback(
        self, 
        enriched_context: Dict[str, Any], 
        underused_types: List[str]
    ) -> Tuple[str, str]:
        """Sele√ß√£o t√©cnica do par quando IA falha."""
        
        unit_type = enriched_context["unit_info"]["unit_type"]
        cefr_level = enriched_context["unit_info"]["cefr_level"]
        
        # Prefer√™ncias por tipo de unidade
        if unit_type == "lexical_unit":
            preferred = ["gap_fill", "matching", "true_false"]
        else:  # grammar_unit
            preferred = ["transformation", "reordering", "cloze_test"]
        
        # Selecionar do underused primeiro
        selected = []
        for pref in preferred:
            if pref in underused_types:
                selected.append(pref)
                if len(selected) == 2:
                    break
        
        # Completar se necess√°rio
        if len(selected) < 2:
            for pref in preferred:
                if pref not in selected:
                    selected.append(pref)
                    if len(selected) == 2:
                        break
        
        # Fallback final
        if len(selected) < 2:
            selected = ["gap_fill", "multiple_choice"]
        
        return tuple(selected[:2])
    
    def _technical_activity_fallback(
        self, 
        activity_type: str, 
        enriched_context: Dict[str, Any],
        activity_number: int
    ) -> AssessmentActivity:
        """Atividade t√©cnica de fallback quando IA falha."""
        
        unit_info = enriched_context["unit_info"]
        content_analysis = enriched_context["content_analysis"]
        vocabulary_words = content_analysis["vocabulary_words"][:3]
        
        # Conte√∫do b√°sico por tipo
        basic_content = self._get_basic_content_for_type(activity_type, vocabulary_words, unit_info["context"])
        
        return AssessmentActivity(
            type=AssessmentType(activity_type),
            title=f"{activity_type.replace('_', ' ').title()} Activity",
            instructions=f"Complete the {activity_type.replace('_', ' ')} exercise using the vocabulary from the unit.",
            content=basic_content,
            answer_key={"1": "answer1", "2": "answer2"},
            estimated_time=12,
            difficulty_level="intermediate",
            skills_assessed=SKILLS_ASSESSED_MAPPING[activity_type],
            vocabulary_focus=vocabulary_words,
            pronunciation_focus=False,
            phonetic_elements=[]
        )
    
    def _get_basic_content_for_type(
        self, 
        activity_type: str, 
        vocabulary_words: List[str], 
        context: str
    ) -> Dict[str, Any]:
        """Gerar conte√∫do b√°sico por tipo de atividade."""
        
        word1 = vocabulary_words[0] if vocabulary_words else "word"
        word2 = vocabulary_words[1] if len(vocabulary_words) > 1 else "example"
        
        content_templates = {
            "cloze_test": {
                "text": f"In the {context}, people often use _____ to communicate effectively. The _____ is very important.",
                "gaps": [word1, word2]
            },
            "gap_fill": {
                "sentences": [f"The _____ is essential in {context}.", f"People need to _____ properly."],
                "options": [[word1, "other", "option"], [word2, "different", "choice"]]
            },
            "reordering": {
                "scrambled_items": [f"{word1} important very is", f"in used {context} {word2}"],
                "correct_order": [0, 1]
            },
            "transformation": {
                "transformations": [
                    {"original": f"The {word1} is important.", "target": f"It's important to use {word1}."},
                    {"original": f"We need {word2}.", "target": f"{word2} is needed."}
                ]
            },
            "multiple_choice": {
                "questions": [
                    {"question": f"What is used in {context}?", "options": [word1, "option2", "option3", "option4"]},
                    {"question": f"How do you {word2}?", "options": ["correctly", "wrong1", "wrong2", "wrong3"]}
                ]
            },
            "true_false": {
                "text": f"In {context}, {word1} is very important. People always use {word2} correctly.",
                "statements": [f"{word1} is important in {context}.", f"Everyone uses {word2} perfectly."]
            },
            "matching": {
                "left_column": [word1, word2, "item3"],
                "right_column": ["definition1", "definition2", "definition3"]
            }
        }
        
        return content_templates.get(activity_type, {"content": "basic activity content"})
    
    def _technical_complementarity_fallback(
        self, 
        activity_1: AssessmentActivity, 
        activity_2: AssessmentActivity
    ) -> Dict[str, Any]:
        """An√°lise t√©cnica de complementaridade quando IA falha."""
        
        # An√°lise t√©cnica simples
        skills_1 = set(activity_1.skills_assessed)
        skills_2 = set(activity_2.skills_assessed)
        
        overlap = len(skills_1 & skills_2) / max(len(skills_1 | skills_2), 1)
        coverage = len(skills_1 | skills_2) / 7  # 7 habilidades principais
        total_time = activity_1.estimated_time + activity_2.estimated_time
        time_appropriate = 0.9 if 15 <= total_time <= 25 else 0.6
        
        complementarity_score = (1 - overlap + coverage + time_appropriate) / 3
        
        return {
            "is_complementary": complementarity_score > 0.6,
            "skill_overlap": overlap,
            "skill_coverage": coverage,
            "time_appropriateness": time_appropriate,
            "complementarity_score": complementarity_score,
            "strengths": ["Atividades com diferentes focos"],
            "potential_gaps": ["Poss√≠vel melhoria na complementaridade"]
        }
    
    # =============================================================================
    # UTILITY METHODS
    # =============================================================================
    
    def _assess_content_richness(self, content_data: Dict[str, Any]) -> float:
        """Avaliar riqueza do conte√∫do dispon√≠vel (an√°lise t√©cnica)."""
        
        richness_score = 0.0
        
        # Vocabul√°rio
        vocab_items = content_data.get("vocabulary", {}).get("items", [])
        if len(vocab_items) >= 20:
            richness_score += 0.3
        elif len(vocab_items) >= 10:
            richness_score += 0.2
        else:
            richness_score += 0.1
        
        # Sentences
        sentences = content_data.get("sentences", {}).get("sentences", [])
        if len(sentences) >= 10:
            richness_score += 0.3
        elif len(sentences) >= 5:
            richness_score += 0.2
        else:
            richness_score += 0.1
        
        # Estrat√©gias
        if content_data.get("tips") or content_data.get("grammar"):
            richness_score += 0.2
        
        # Q&A
        if content_data.get("qa"):
            richness_score += 0.2
        
        return min(richness_score, 1.0)
    
    def _calculate_updated_balance(
        self, 
        current_usage: Dict[str, int], 
        selected_pair: Tuple[str, str]
    ) -> Dict[str, Any]:
        """Calcular balanceamento atualizado ap√≥s sele√ß√£o."""
        
        updated_usage = current_usage.copy()
        
        for activity_type in selected_pair:
            updated_usage[activity_type] = updated_usage.get(activity_type, 0) + 1
        
        total_activities = sum(updated_usage.values())
        distribution = {k: v/total_activities for k, v in updated_usage.items()} if total_activities > 0 else {}
        
        # Calcular novo score de balanceamento
        ideal_distribution = 1.0 / len(ASSESSMENT_TYPES)  # ~0.143
        variance = sum((freq - ideal_distribution) ** 2 for freq in distribution.values())
        balance_score = max(0.0, 1.0 - variance * 5)  # Escalar para 0-1
        
        return {
            "updated_distribution": updated_usage,
            "balance_score": balance_score,
            "total_activities": total_activities,
            "most_used": max(updated_usage.items(), key=lambda x: x[1]) if updated_usage else None,
            "least_used": min(updated_usage.items(), key=lambda x: x[1]) if updated_usage else None
        }
    
    async def get_service_status(self) -> Dict[str, Any]:
        """Obter status do servi√ßo."""
        return {
            "service": "AssessmentSelectorService", 
            "status": "active",
            "assessment_types": ASSESSMENT_TYPES,
            "total_types_available": len(ASSESSMENT_TYPES),
            "ai_integration": "100% contextual analysis",
            "rag_balancing": "enabled",
            "complementarity_analysis": "enabled",
            "cefr_adaptation": "enabled",
            "selection_method": "intelligent_pair_selection",
            "ai_analysis_methods": [
                "_analyze_current_balance_ai",
                "_identify_underused_assessments_ai",
                "_select_complementary_pair_ai",
                "_generate_specific_activity_ai",
                "_generate_selection_rationale_ai",
                "_analyze_complementarity_ai"
            ],
            "constants_maintained": list(ASSESSMENT_DESCRIPTIONS.keys())
        }
    
    async def validate_selection_params(self, params: Dict[str, Any]) -> Dict[str, Any]:
        """Validar par√¢metros de sele√ß√£o."""
        validation_result = {
            "valid": True,
            "errors": [],
            "warnings": []
        }
        
        # Valida√ß√µes b√°sicas
        required_fields = ["unit_data", "content_data", "hierarchy_context", "rag_context"]
        for field in required_fields:
            if field not in params:
                validation_result["errors"].append(f"Campo obrigat√≥rio ausente: {field}")
                validation_result["valid"] = False
        
        # Valida√ß√µes espec√≠ficas
        unit_data = params.get("unit_data", {})
        if not unit_data.get("cefr_level"):
            validation_result["warnings"].append("N√≠vel CEFR n√£o especificado")
        
        content_data = params.get("content_data", {})
        if not content_data.get("vocabulary", {}).get("items"):
            validation_result["warnings"].append("Vocabul√°rio n√£o dispon√≠vel - pode afetar qualidade")
        
        rag_context = params.get("rag_context", {})
        if not rag_context.get("used_assessments"):
            validation_result["warnings"].append("Hist√≥rico de atividades vazio - primeira unidade?")
        
        return validation_result


# =============================================================================
# UTILITY FUNCTIONS PARA ENDPOINTS
# =============================================================================

async def select_assessments_for_unit_creation(selection_params: Dict[str, Any]) -> Dict[str, Any]:
    """
    Fun√ß√£o de conveni√™ncia para sele√ß√£o de atividades em endpoints V2.
    Mant√©m compatibilidade com a API existente.
    """
    try:
        service = AssessmentSelectorService()
        assessment_section = await service.select_optimal_assessments(selection_params)
        
        return {
            "success": True,
            "assessment_section": assessment_section.dict(),
            "selection_time": time.time(),
            "service_version": "langchain_0.3_pydantic_2_rag_balanced"
        }
        
    except Exception as e:
        logger.error(f"‚ùå Erro na fun√ß√£o de conveni√™ncia: {str(e)}")
        return {
            "success": False,
            "error": str(e),
            "fallback_available": True
        }


async def analyze_assessment_balance_ai(
    used_assessments: Dict[str, int],
    unit_context: str,
    cefr_level: str
) -> Dict[str, Any]:
    """
    Analisar balanceamento de atividades via IA.
    
    Args:
        used_assessments: Hist√≥rico de uso de atividades
        unit_context: Contexto da unidade
        cefr_level: N√≠vel CEFR
        
    Returns:
        An√°lise de balanceamento
    """
    service = AssessmentSelectorService()
    
    # Construir contexto m√≠nimo
    enriched_context = {
        "unit_info": {"context": unit_context, "cefr_level": cefr_level},
        "rag_analysis": {"used_assessments": used_assessments},
        "hierarchy_info": {"sequence_order": sum(used_assessments.values())}
    }
    
    try:
        balance_analysis = await service._analyze_current_balance_ai(enriched_context)
        return balance_analysis
    except Exception as e:
        logger.warning(f"Erro na an√°lise de balanceamento via IA: {str(e)}")
        return service._technical_balance_analysis_fallback(enriched_context)


def calculate_assessment_distribution_metrics(used_assessments: Dict[str, int]) -> Dict[str, Any]:
    """
    Calcular m√©tricas de distribui√ß√£o de atividades (an√°lise t√©cnica).
    
    Args:
        used_assessments: Hist√≥rico de uso
        
    Returns:
        M√©tricas de distribui√ß√£o
    """
    if not used_assessments:
        return {
            "total_activities": 0,
            "unique_types": 0,
            "most_used": None,
            "least_used": None,
            "balance_score": 1.0,
            "diversity_score": 0.0
        }
    
    total = sum(used_assessments.values())
    unique_types = len(used_assessments)
    
    # Encontrar mais e menos usado
    most_used = max(used_assessments.items(), key=lambda x: x[1])
    least_used = min(used_assessments.items(), key=lambda x: x[1])
    
    # Calcular scores
    max_ideal = total / len(ASSESSMENT_TYPES)
    balance_score = 1.0 - (most_used[1] - max_ideal) / max(total, 1)
    diversity_score = unique_types / len(ASSESSMENT_TYPES)
    
    return {
        "total_activities": total,
        "unique_types": unique_types,
        "most_used": {"type": most_used[0], "count": most_used[1]},
        "least_used": {"type": least_used[0], "count": least_used[1]},
        "balance_score": max(0.0, balance_score),
        "diversity_score": diversity_score,
        "distribution": {k: v/total for k, v in used_assessments.items()}
    }

    # =============================================================================
    # M√âTODO PRINCIPAL PARA API (ADICIONADO PARA CORRE√á√ÉO)
    # =============================================================================
    
    async def generate_assessments_for_unit(self, assessment_params: Dict[str, Any]) -> AssessmentSection:
        """
        M√©todo principal que o endpoint chama para gerar assessments.
        
        Args:
            assessment_params: Par√¢metros completos do assessment
            
        Returns:
            AssessmentSection: Se√ß√£o de assessments gerada
        """
        return await self.select_optimal_assessments(assessment_params)


def get_assessment_recommendations_for_cefr(cefr_level: str, unit_type: str) -> List[str]:
    """
    Obter recomenda√ß√µes de atividades por n√≠vel CEFR e tipo de unidade.
    
    Args:
        cefr_level: N√≠vel CEFR (A1, A2, B1, B2, C1, C2)
        unit_type: Tipo de unidade (lexical_unit, grammar_unit)
        
    Returns:
        Lista de tipos recomendados
    """
    base_recommendations = CEFR_ASSESSMENT_PREFERENCES.get(cefr_level, ASSESSMENT_TYPES[:4])
    
    # Ajustar por tipo de unidade
    if unit_type == "lexical_unit":
        # Priorizar atividades focadas em vocabul√°rio
        lexical_priority = ["gap_fill", "matching", "true_false", "cloze_test"]
        recommendations = [t for t in lexical_priority if t in base_recommendations]
        recommendations.extend([t for t in base_recommendations if t not in recommendations])
    
    elif unit_type == "grammar_unit":
        # Priorizar atividades focadas em estruturas
        grammar_priority = ["transformation", "reordering", "cloze_test", "gap_fill"]
        recommendations = [t for t in grammar_priority if t in base_recommendations] 
        recommendations.extend([t for t in base_recommendations if t not in recommendations])
    
    else:
        recommendations = base_recommendations
    
    return recommendations[:4]


def create_assessment_variety_report(used_assessments: Dict[str, int]) -> str:
    """
    Criar relat√≥rio de variedade de atividades.
    
    Args:
        used_assessments: Hist√≥rico de uso
        
    Returns:
        Relat√≥rio formatado
    """
    if not used_assessments:
        return "ASSESSMENT VARIETY REPORT\n\nNo activities used yet. Ready to start with balanced selection."
    
    metrics = calculate_assessment_distribution_metrics(used_assessments)
    
    report_lines = [
        "ASSESSMENT VARIETY REPORT",
        "=" * 30,
        "",
        f"Total Activities: {metrics['total_activities']}",
        f"Unique Types Used: {metrics['unique_types']}/{len(ASSESSMENT_TYPES)}",
        f"Balance Score: {metrics['balance_score']:.1%}",
        f"Diversity Score: {metrics['diversity_score']:.1%}",
        "",
        "DISTRIBUTION:",
    ]
    
    # Ordenar por uso
    sorted_usage = sorted(used_assessments.items(), key=lambda x: x[1], reverse=True)
    
    for activity_type, count in sorted_usage:
        percentage = (count / metrics['total_activities']) * 100
        bar = "‚ñà" * int(percentage / 5)  # Visual bar
        description = ASSESSMENT_DESCRIPTIONS.get(activity_type, "Unknown activity")
        report_lines.append(f"  {activity_type:15} [{count:2}] {bar:10} {percentage:5.1f}% - {description}")
    
    # Tipos n√£o usados
    unused_types = [t for t in ASSESSMENT_TYPES if t not in used_assessments]
    if unused_types:
        report_lines.extend([
            "",
            "UNUSED TYPES:",
            ", ".join(unused_types)
        ])
    
    # Recomenda√ß√µes
    if metrics['balance_score'] < 0.7:
        report_lines.extend([
            "",
            "RECOMMENDATIONS:",
            f"‚Ä¢ Consider using {metrics['least_used']['type']} more frequently",
            f"‚Ä¢ Reduce usage of {metrics['most_used']['type']} temporarily", 
            "‚Ä¢ Aim for more balanced distribution"
        ])
    else:
        report_lines.extend([
            "",
            "‚úÖ Good variety and balance maintained!"
        ])
    
    return "\n".join(report_lines)